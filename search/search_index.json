{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"The Chap Modeling Platform","text":"<p>Chap is a Climate &amp; Health Modeling Platform that brings together disease forecasting models into a unified ecosystem, connecting researchers with cutting-edge epidemiological models to policy makers and health practitioners. It makes complex modeling workflows accessible to users, automates rigorous model evaluation, provides a broad range of generic convenience functionality available to modellers, and integrates directly with DHIS2, the world's leading health information system.</p> <p>We here provide technical documentation tailored to:</p> <ul> <li>Model developers</li> <li>Platform contributors</li> <li>System administrators wanting to set up Chap with connections to DHIS2 and the Modeling App</li> </ul> <p>For more general information about the Chap project, we refer you to the Chap Modelling Platform wiki.</p>"},{"location":"changelog.html","title":"Releases and Future Plans","text":"<p>In the current phase of development, the <code>chap-core</code> Python package is released frequently.</p> <p>The current version and notes on changes can be found on the GitHub release page.</p> <p>A detailed overview of tasks we are working on can be found on our open project board.</p>"},{"location":"features_overview.html","title":"Current and planned functionality and resources for modellers in Chap","text":"<p>The Chap modelling platform brings together a broad range of functionality for streamlining climate health modelling into a unified ecosystem. This document provides an overview of existing and planned functionality and features, mainly meant for model developers and Chap developers.</p>"},{"location":"features_overview.html#at-the-core-of-chap-is-the-plugin-like-support-for-incorporating-models-into-the-platform","title":"At the core of Chap is the plugin-like support for incorporating models into the platform:","text":"<ul> <li>This is based on a scheme where a model provides functionality to train a model and predict ahead in time, defining its entry points in an MLFlow-based format.</li> <li>Any model adhering to this can be used in the platform by having the model available as a github repository and providing the url for this repo to the Chap platform. </li> <li>Chap can run the model either in its own python environment or in a docker container (where the model points to a docker image that it can be run within).</li> <li>Model developers are offered a template (in Python or R) or minimalist example of a working model codebase to start from (in Python or R).</li> <li>There is ongoing work to change from the current minimalist examples to a more sophisticated starting point based on an SDK in Python and R. </li> <li>There is also ongoing work to change from the current scheme of each model being run as a subprocess (possibly within a Docker container) to instead having the models provide a REST API for communication with the Chap Platform (the chapkit project </li> </ul> <p>Integrating with the Chap platform allows to focus only on the model itself, and by having it adhere to our standard interface the model can rely on the platform for the central aspects of data input, ways of running models, model evaluation and optional DHIS2 integration.</p>"},{"location":"features_overview.html#data-input","title":"Data input:","text":"<ul> <li>Chap allows input from a well-defined csv format for harmonised climate and health data.</li> <li>A broad repository of public harmonised climate and health data is available in this format and can be directly used with a model. </li> <li>There are also future plans for a collection of metadata for restricted data potentially available from specific countries (TODO: and more?).</li> <li>There is ongoing work on generation of synthetic climate and health datasets for understanding model behaviour and stress-testing model in particular settings. </li> <li>There is ongoing work on supporting the computation of endemic channels (outbreak thresholds), as well as functionality to compute outbreak periods (binary representation of early warning forecast) based on outbreak threshold and probabilistic disease forecasts.  </li> </ul>"},{"location":"features_overview.html#ways-of-running-models","title":"Ways of running models:","text":"<ul> <li>Any model can always be run through its native programming language, and the ongoing work on SDKs will bring streamlined ways of running</li> <li>Any model, implemented in any language, can be run through the Chap command-line interface</li> <li>There is ongoing work on streamlined REST api setup for methods, allowing any model to also be run through a REST api (including over the internet??)</li> <li>As described below, through optional streamlined DHIS2 connection, a model can be run through a GUI using the Modelling App   </li> </ul>"},{"location":"features_overview.html#rigorous-evaluation-of-model-predictions","title":"Rigorous evaluation of model predictions:","text":"<ul> <li>Model predictions can be contrasted to truth according to our precisely defined (TODO) evaluation scheme that follows our dogma of what constitutes appropriate evaluation.</li> <li>There is ongoing work on a benchmarking server that streamlines extensive evaluation and continuous evaluation through development.</li> <li>There are future plans for federated model evaluation through Chap, in which a model can be evaluated on data across multiple countries without needing to be provided access to the data itself for these countries</li> <li>Plans for a standard benchmark setup that allows any model integrated with Chap to be assessed on a standard collection of data using a standard collection of metrics and visualisations  </li> </ul>"},{"location":"features_overview.html#chap-further-includes-optional-streamlined-setup-of-connection-to-dhis-which-provides-the-following-additional-features","title":"Chap further includes optional streamlined setup of connection to DHIS, which provides the following additional features:","text":"<ul> <li>Direct data input from DHIS2, which through the Climate App and Climate Tools may contain up-to-date, harmonised climate and health data according to well-defined criteria.</li> <li>Direct dissemination of predictions back to DHIS2</li> <li>Using/offering the Modelling App as a GUI for your own and reference models: Configuring, tuning, training, evaluating and predicting with models, as well as visualising data, model predictions and evaluations.   </li> <li> <p>Interoperability with the full set of DHIS2 ecosystem tools and functionalities, including planned support for missing data analysis and imputation, for endemic threshold definition and outbreak inference, for derived variable computation and dashboard visualisation of prediction.</p> </li> <li> <p>In addition to the plugin-like system for models, we similarly offer:</p> </li> <li>A plugin-like system for evaluation metrics, allowing anyone to contribute implementations of custom metrics (formulas) for evaluating model predictions against truth   </li> <li>A plugin-like system for visualisations, allowing anyone to contribute visualisations of data or visualisations for model evaluation.  </li> </ul>"},{"location":"features_overview.html#beyond-the-core-features-described-above-the-platform-also-currently-or-in-the-future-offers-the-following-features-to-any-model-integrated-with-it","title":"Beyond the core features described above, the platform also currently or in the future offers the following features to any model integrated with it","text":"<ul> <li>Persistency: Both trained models and their predictions on different datasets can be stored according to our persistency support, allowing to run trained models on new data and set up comparative evaluations. </li> <li>AutoML: There is ongoing work to support automatic model tuning (model Hyper-parameters to be tuned), as well as planned work to allow automatic variable selection and automatic selection of model to use on a given dataset.</li> <li>Ensemble model learning: There is ongoing work on combining multiple models to get both mean predictions and uncertainty combined across models, ranging from: </li> <li>Ongoing work on basic aggregation and manual or automatically learned weighting of multiple models </li> <li>Plans for an adaptive ensemble approach (mixture of experts), where model choice within an ensemble is dynamically set based on data</li> <li>Model introspection and explainability:<ul> <li>Ongoing work on an open way for models to provide any information on a trained model or model predictions </li> <li>Ongoing work on a generic ontology and protocol for models to communicate model properties (like variable importance) in a way that can be easily compared across models</li> <li>Planned work on a generic data perturbation scheme to infer model (across-model comparable) characteristics (like variable importance) from the platform side through the standard train and prediction endpoints (i.e. without models having to implement anything related to model introspection/explainability)   </li> <li>Planned work on missing data sensitivity analysis (by randomly dropping data and assessing its effect)</li> <li>Planned work on automatic brokering of compatible models for a given prediction context according to metadata (filtering models based on chosen data availability and decision need)</li> </ul> </li> <li>Plans for overall summary of forecasting analyses, including details of data, training and prediction skill</li> </ul>"},{"location":"features_overview.html#research","title":"Research","text":"<ul> <li>We have many ambitions on research and scientific publications on technical, IS and climate health aspects of Chap  </li> </ul>"},{"location":"features_overview.html#documentation-tutorials-and-capacity-development","title":"Documentation, tutorials and capacity development","text":"<ul> <li>We provide an overall Chap documentation, with subparts for </li> <li>How to learn about and integrate with Chap as modeller </li> <li>How to contribute to the core Chap codebase. </li> <li>We provide capacity building material on learning modelling based on Chap</li> <li>We have a separate tutorial meant for master students or similar to get started with Chap. </li> </ul>"},{"location":"features_overview.html#collaboration-and-supervision","title":"Collaboration and supervision","text":"<ul> <li>The following PhD students do their PhD project with Chap being central:</li> <li>Herman Tretteteig</li> <li>Halvard Emil Sand-Larsen</li> <li>The following Master students are currently working on concrete aspects of Chap:</li> <li>Lilu Zhan: autoML (tutorial)</li> <li>Nora Jeanett T\u00f8nnessen: modularised visualisation (tutorial)</li> <li>Markus Byrkjedal Slyngstad: model cards (tutorial)</li> <li>Behdad Nikkhah: ensemble learning (tutorial)</li> <li>Hamed Hussaini: model introspection (tutorial)</li> <li>Ali Hassan: federated model evaluation (tutorial)</li> <li>Andre Maharaj Gregussen: modularised evaluation metric definition (tutorial)</li> <li>The following Master students are planned to contribute to Chap in the time ahead:</li> <li>Leander S. Parton, Hans Andersen, August Aspelien, William Henrik Behn, Ole Martin Skovly  Henning, Sigurd Smeby, Aulona Sylanaj</li> <li>The following Master students have delivered a thesis connected to Chap in the past:</li> <li>Martin Hansen Bolle and Ingar Andre Benonisen: synthetic datasets</li> <li>The following external collaborators have contributed to Chap:</li> <li>Harsha Halgamuwe Hewage: The use of Chap for drug logistics planning </li> </ul>"},{"location":"api/index.html","title":"API Reference","text":"<p>This section provides auto-generated API documentation from the chap_core Python package.</p>"},{"location":"api/index.html#core-modules","title":"Core Modules","text":""},{"location":"api/index.html#assessment","title":"Assessment","text":""},{"location":"api/index.html#chap_core.assessment","title":"<code>assessment</code>","text":""},{"location":"api/index.html#models","title":"Models","text":""},{"location":"api/index.html#chap_core.models","title":"<code>models</code>","text":""},{"location":"api/index.html#runners","title":"Runners","text":""},{"location":"api/index.html#chap_core.runners","title":"<code>runners</code>","text":""},{"location":"api/index.html#database","title":"Database","text":""},{"location":"api/index.html#chap_core.database","title":"<code>database</code>","text":""},{"location":"api/index.html#data-types","title":"Data Types","text":""},{"location":"api/index.html#chap_core.datatypes","title":"<code>datatypes</code>","text":""},{"location":"api/index.html#chap_core.datatypes.TimeSeriesData","title":"<code>TimeSeriesData</code>","text":"Source code in <code>chap_core/datatypes.py</code> <pre><code>@tsdataclass\nclass TimeSeriesData:\n    time_period: PeriodRange\n\n    def model_dump(self):\n        return {field.name: getattr(self, field.name).tolist() for field in dataclasses.fields(self)}\n\n    def __getstate__(self):\n        return self.todict()\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n\n    def join(self, other):\n        return np.concatenate([self, other])\n\n    def resample(self, freq):\n        df = self.topandas()\n        df[\"time_period\"] = self.time_period.to_period_index()\n        df = df.set_index(\"time_period\")\n        df = df.resample(freq).interpolate()\n        return self.from_pandas(df.reset_index())\n\n    def topandas(self):\n        data_dict = {field.name: getattr(self, field.name) for field in dataclasses.fields(self)}\n        for key, value in data_dict.items():\n            if isinstance(value, np.ndarray) and value.ndim &gt; 1:\n                data_dict[key] = value.tolist()\n        data_dict[\"time_period\"] = self.time_period.topandas()\n        return pd.DataFrame(data_dict)\n\n    to_pandas = topandas\n\n    def to_csv(self, csv_file: str, **kwargs):\n        \"\"\"Write data to a csv file.\"\"\"\n        data = self.to_pandas()\n        data.to_csv(csv_file, index=False, **kwargs)\n\n    def to_pickle_dict(self):\n        data_dict = {field.name: getattr(self, field.name) for field in dataclasses.fields(self)}\n        data_dict[\"time_period\"] = self.time_period.tolist()\n        return data_dict\n\n    @classmethod\n    def from_pickle_dict(cls, data: dict):\n        return cls(\n            **{key: PeriodRange.from_strings(value) if key == \"time_period\" else value for key, value in data.items()}\n        )\n\n    @classmethod\n    def create_class_from_basemodel(cls, dataclass: type[PeriodObservation]):\n        fields = dataclass.model_fields\n        fields = [\n            (name, field.annotation) if name != \"time_period\" else (name, PeriodRange) for name, field in fields.items()\n        ]\n        return dataclasses.make_dataclass(dataclass.__name__, fields, bases=(TimeSeriesData,))\n\n    @staticmethod\n    def _fill_missing(data, missing_indices):\n        if len(missing_indices) == 0:\n            return data\n        n_entries = len(data) + len(missing_indices)\n        filled_data = np.full(n_entries, np.nan)\n        mask = np.full(n_entries, True)\n        mask[missing_indices] = False\n        filled_data[mask] = data\n        return filled_data\n\n    @classmethod\n    def from_pandas(cls, data: pd.DataFrame, fill_missing: bool = False) -&gt; \"TimeSeriesData\":\n        try:\n            time_strings = data.time_period.astype(str)\n            # check unique\n            assert len(time_strings) == len(set(time_strings)), f\"{time_strings} has duplicates\"\n            time = PeriodRange.from_strings(time_strings, fill_missing=fill_missing)\n        except Exception:\n            print(\"Error in time period: \", data.time_period)\n            raise\n\n        if fill_missing:\n            time, missing_indices = time\n            mask = np.full(len(time), True)\n            mask[missing_indices] = False\n        else:\n            missing_indices = []\n        # time = parse_periods_strings(data.time_period.astype(str))\n        variable_names = [field.name for field in dataclasses.fields(cls) if field.name != \"time_period\"]\n        data = [cls._fill_missing(data[name].values, missing_indices) for name in variable_names]\n        assert all(len(d) == len(time) for d in data), f\"{[len(d) for d in data]} != {len(time)}\"\n        return cls(time, **dict(zip(variable_names, data)))\n\n    @classmethod\n    def from_csv(cls, csv_file: str, **kwargs):\n        \"\"\"Read data from a csv file.\"\"\"\n        data = pd.read_csv(csv_file, **kwargs)\n        return cls.from_pandas(data)\n\n    def interpolate(self, field_names: Optional[List[str]] = None):\n        data_dict = {field.name: getattr(self, field.name) for field in dataclasses.fields(self)}\n        data_dict[\"time_period\"] = self.time_period\n        fields = {\n            key: interpolate_nans(value)\n            if ((field_names is None) or (key in field_names) and not np.all(np.isnan(value)))\n            else value\n            for key, value in data_dict.items()\n            if key != \"time_period\"\n        }\n        return self.__class__(self.time_period, **fields)\n\n    @deprecated(\"Compatibility with old code\")\n    def data(self):\n        return self\n\n    @property\n    def start_timestamp(self) -&gt; pd.Timestamp:\n        return self.time_period[0].start_timestamp\n\n    @property\n    def end_timestamp(self) -&gt; pd.Timestamp:\n        return self.time_period[-1].end_timestamp\n\n    def fill_to_endpoint(self, end_time_stamp: TimeStamp) -&gt; \"TimeSeriesData\":\n        if self.end_timestamp == end_time_stamp:\n            return self\n        n_missing = (end_time_stamp - self.end_timestamp) // self.time_period.delta\n        assert n_missing &gt;= 0, (f\"{n_missing} &lt; 0\", end_time_stamp, self.end_timestamp)\n        old_time_period = self.time_period\n        new_time_period = PeriodRange(old_time_period.start_timestamp, end_time_stamp, old_time_period.delta)\n        d = {field.name: getattr(self, field.name) for field in dataclasses.fields(self) if field.name != \"time_period\"}\n\n        for name, data in d.items():\n            d[name] = np.pad(data.astype(float), (0, n_missing), constant_values=np.nan)\n        return self.__class__(new_time_period, **d)\n\n    def fill_to_range(self, start_timestamp, end_timestamp):\n        if self.end_timestamp == end_timestamp and self.start_timestamp == start_timestamp:\n            return self\n        n_missing_start = self.time_period.delta.n_periods(start_timestamp, self.start_timestamp)\n        # (self.start_timestamp - start_timestamp) // self.time_period.delta\n        n_missing = self.time_period.delta.n_periods(self.end_timestamp, end_timestamp)\n        # n_missing = (end_timestamp - self.end_timestamp) // self.time_period.delta\n        assert n_missing &gt;= 0, (f\"{n_missing} &lt; 0\", end_timestamp, self.end_timestamp)\n        assert n_missing_start &gt;= 0, (\n            f\"{n_missing} &lt; 0\",\n            end_timestamp,\n            self.end_timestamp,\n        )\n        old_time_period = self.time_period\n        new_time_period = PeriodRange(start_timestamp, end_timestamp, old_time_period.delta)\n        d = {field.name: getattr(self, field.name) for field in dataclasses.fields(self) if field.name != \"time_period\"}\n\n        for name, data in d.items():\n            d[name] = np.pad(data.astype(float), (n_missing_start, n_missing), constant_values=np.nan)\n        return self.__class__(new_time_period, **d)\n\n    def to_array(self):\n        return np.array(\n            [getattr(self, field.name) for field in dataclasses.fields(self) if field.name != \"time_period\"]\n        ).T\n\n    def todict(self):\n        d = super().todict()\n        d[\"time_period\"] = self.time_period.topandas()\n        return d\n\n    @classmethod\n    def from_dict(cls, data: dict):\n        return cls(\n            **{key: PeriodRange.from_strings(value) if key == \"time_period\" else value for key, value in data.items()}\n        )\n\n    def merge(self, other: \"TimeSeriesData\", result_class: type[\"TimeSeriesData\"]):\n        data_dict = {}\n        if len(self.time_period) != len(other.time_period) or np.any(self.time_period != other.time_period):\n            raise ValueError(f\"{self.time_period} != {other.time_period}\")\n        for field in dataclasses.fields(result_class):\n            field_name = field.name\n            if field_name == \"time_period\":\n                continue\n            if hasattr(self, field_name):\n                assert not hasattr(other, field_name), f\"Field {field_name} in both data\"\n                data_dict[field_name] = getattr(self, field_name)\n            elif hasattr(other, field_name):\n                data_dict[field_name] = getattr(other, field_name)\n            else:\n                raise ValueError(f\"Field {field_name} not in either data\")\n        return result_class(self.time_period, **data_dict)\n</code></pre>"},{"location":"api/index.html#chap_core.datatypes.TimeSeriesData.to_csv","title":"<code>to_csv(csv_file, **kwargs)</code>","text":"<p>Write data to a csv file.</p> Source code in <code>chap_core/datatypes.py</code> <pre><code>def to_csv(self, csv_file: str, **kwargs):\n    \"\"\"Write data to a csv file.\"\"\"\n    data = self.to_pandas()\n    data.to_csv(csv_file, index=False, **kwargs)\n</code></pre>"},{"location":"api/index.html#chap_core.datatypes.TimeSeriesData.from_csv","title":"<code>from_csv(csv_file, **kwargs)</code>  <code>classmethod</code>","text":"<p>Read data from a csv file.</p> Source code in <code>chap_core/datatypes.py</code> <pre><code>@classmethod\ndef from_csv(cls, csv_file: str, **kwargs):\n    \"\"\"Read data from a csv file.\"\"\"\n    data = pd.read_csv(csv_file, **kwargs)\n    return cls.from_pandas(data)\n</code></pre>"},{"location":"api/index.html#spatio-temporal-data","title":"Spatio-Temporal Data","text":""},{"location":"api/index.html#chap_core.spatio_temporal_data","title":"<code>spatio_temporal_data</code>","text":""},{"location":"api/index.html#climate-data","title":"Climate Data","text":""},{"location":"api/index.html#chap_core.climate_data","title":"<code>climate_data</code>","text":""},{"location":"api/index.html#file-io","title":"File I/O","text":""},{"location":"api/index.html#chap_core.file_io","title":"<code>file_io</code>","text":""},{"location":"chap-cli/index.html","title":"Using the CLI Tool","text":"<p>The CHAP CLI provides commands for evaluating disease prediction models, visualizing results, and exporting metrics.</p>"},{"location":"chap-cli/index.html#quick-start","title":"Quick Start","text":"<p>The main workflow consists of three commands:</p> <ol> <li><code>chap evaluate2</code> - Run a backtest and export results to NetCDF format</li> <li><code>chap plot-backtest</code> - Generate visualizations from evaluation results</li> <li><code>chap export-metrics</code> - Export and compare metrics across evaluations</li> </ol> <p>See the Evaluation Workflow guide for detailed usage and examples.</p>"},{"location":"chap-cli/index.html#documentation","title":"Documentation","text":"<ul> <li>Setup - How to install and configure the CLI</li> <li>Evaluation Workflow - Complete guide to evaluating and comparing models</li> </ul>"},{"location":"chap-cli/chap-core-cli-setup.html","title":"Setting up CHAP Core CLI Tool","text":"<p>If you want to use CHAP Core on the command line, develop custom models, or integrate external forecasting models with CHAP, you should install the <code>chap-core</code> Python package.</p> <p>Important: This guide is for end-users who need a stable version of CHAP Core. If you are a developer and want to make changes or contribute to the CHAP Core codebase, follow the getting started guide for contributors instead.</p>"},{"location":"chap-cli/chap-core-cli-setup.html#installation","title":"Installation","text":"<p>We recommend using uv for installation. If you don't have uv installed, you can install it with:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>On Windows, use:</p> <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <p>Then install <code>chap-core</code>:</p> <pre><code>uv tool install chap-core\n</code></pre> <p>To install a specific version (e.g., v1.0.1):</p> <pre><code>uv tool install chap-core==1.0.1\n</code></pre>"},{"location":"chap-cli/chap-core-cli-setup.html#verify-installation","title":"Verify Installation","text":"<p>To verify that the installation worked, check that the <code>chap</code> command is available:</p> <pre><code>chap --help\n</code></pre> <p>You should see output listing available commands including <code>evaluate2</code>, <code>plot-backtest</code>, and <code>export-metrics</code>.</p>"},{"location":"chap-cli/chap-core-cli-setup.html#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Evaluation Workflow guide to evaluate and compare models</li> </ul>"},{"location":"chap-cli/evaluation-workflow.html","title":"Evaluation Workflow: Comparing Models with CLI","text":"<p>This guide walks through the complete workflow for evaluating models, visualizing results, and comparing metrics using the CHAP CLI.</p>"},{"location":"chap-cli/evaluation-workflow.html#overview","title":"Overview","text":"<p>The workflow consists of three main steps:</p> <ol> <li>evaluate2: Run a backtest and export results to NetCDF format</li> <li>plot-backtest: Generate visualizations from evaluation results</li> <li>export-metrics: Compare metrics across multiple evaluations in CSV format</li> </ol>"},{"location":"chap-cli/evaluation-workflow.html#prerequisites","title":"Prerequisites","text":"<ul> <li>CHAP Core installed (see Setup guide)</li> <li>A dataset CSV file with disease case data</li> <li>A GeoJSON file with region polygons (optional, auto-discovered if named same as CSV)</li> </ul>"},{"location":"chap-cli/evaluation-workflow.html#verify-installation","title":"Verify Installation","text":"<p>Before starting, verify that the CLI tools are installed correctly:</p> <pre><code>chap evaluate2 --help\n</code></pre> <pre><code>chap plot-backtest --help\n</code></pre> <pre><code>chap export-metrics --help\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#example-dataset","title":"Example Dataset","text":"<p>CHAP includes a small example dataset for testing and learning:</p> <ul> <li><code>example_data/laos_subset.csv</code> - Monthly dengue data for 3 provinces (2010-2012)</li> <li><code>example_data/laos_subset.geojson</code> - Matching polygon boundaries</li> </ul> <p>This dataset contains 108 rows with rainfall, temperature, disease cases, and population data for Bokeo, Vientiane, and Savannakhet provinces.</p>"},{"location":"chap-cli/evaluation-workflow.html#step-1-create-an-evaluation","title":"Step 1: Create an Evaluation","text":"<p>Use <code>evaluate2</code> to run a backtest on a model and export results to NetCDF format.</p>"},{"location":"chap-cli/evaluation-workflow.html#standard-models-github-url-or-local-directory","title":"Standard Models (GitHub URL or Local Directory)","text":"<p>For models hosted on GitHub or cloned locally:</p> <pre><code>chap evaluate2 \\\n    --model-name https://github.com/dhis2-chap/minimalist_example_r \\\n    --dataset-csv ./data/vietnam_data.csv \\\n    --output-file ./results/model_a_eval.nc \\\n    --backtest-params.n-periods 3 \\\n    --backtest-params.n-splits 7\n</code></pre> <p>Or using a local directory:</p> <pre><code>chap evaluate2 \\\n    --model-name /path/to/minimalist_example_r \\\n    --dataset-csv ./data/vietnam_data.csv \\\n    --output-file ./results/model_a_eval.nc \\\n    --backtest-params.n-periods 3 \\\n    --backtest-params.n-splits 7\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#chapkit-models","title":"Chapkit Models","text":"<p>Chapkit models are REST API-based models that follow the chapkit specification. See Running models with chapkit for more details.</p> <p>From a running chapkit service (URL):</p> <pre><code>chap evaluate2 \\\n    --model-name http://localhost:8000 \\\n    --dataset-csv ./data/vietnam_data.csv \\\n    --output-file ./results/chapkit_eval.nc \\\n    --run-config.is-chapkit-model \\\n    --backtest-params.n-periods 3 \\\n    --backtest-params.n-splits 7\n</code></pre> <p>From a local chapkit model directory (auto-starts the service):</p> <p>When you provide a directory path with <code>--run-config.is-chapkit-model</code>, CHAP automatically:</p> <ol> <li>Starts a FastAPI dev server from the model directory using <code>uv run fastapi dev</code></li> <li>Waits for the service to become healthy</li> <li>Runs the evaluation</li> <li>Stops the service when complete</li> </ol> <pre><code>chap evaluate2 \\\n    --model-name /path/to/your/chapkit/model \\\n    --dataset-csv ./data/vietnam_data.csv \\\n    --output-file ./results/chapkit_eval.nc \\\n    --run-config.is-chapkit-model \\\n    --backtest-params.n-periods 3 \\\n    --backtest-params.n-splits 7\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#parameters","title":"Parameters","text":"Parameter Description Default <code>--model-name</code> Model path, GitHub URL, or chapkit service URL Required <code>--dataset-csv</code> Path to CSV with disease data Required <code>--output-file</code> Path for output NetCDF file Required <code>--backtest-params.n-periods</code> Forecast horizon (periods ahead) 3 <code>--backtest-params.n-splits</code> Number of train/test splits 7 <code>--backtest-params.stride</code> Step size between splits 1 <code>--model-configuration-yaml</code> Optional YAML with model config None <code>--run-config.is-chapkit-model</code> Flag to indicate chapkit model false <code>--run-config.ignore-environment</code> Skip environment setup false <code>--run-config.debug</code> Enable debug logging false <code>--run-config.run-directory-type</code> Directory handling: <code>latest</code>, <code>timestamp</code>, or <code>use_existing</code> timestamp <code>--historical-context-years</code> Years of historical data for plot context 6"},{"location":"chap-cli/evaluation-workflow.html#geojson-auto-discovery","title":"GeoJSON Auto-Discovery","text":"<p>If your dataset is <code>vietnam_data.csv</code>, CHAP will automatically look for <code>vietnam_data.geojson</code> in the same directory.</p>"},{"location":"chap-cli/evaluation-workflow.html#step-2-visualize-the-evaluation","title":"Step 2: Visualize the Evaluation","text":"<p>Use <code>plot-backtest</code> to generate visualizations from the evaluation results:</p> <pre><code>chap plot-backtest \\\n    --input-file ./results/model_a_eval.nc \\\n    --output-file ./results/model_a_plot.html \\\n    --plot-type backtest_plot_1\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#available-plot-types","title":"Available Plot Types","text":"Plot Type Description <code>backtest_plot_1</code> Standard backtest visualization with forecasts vs observations <code>evaluation_plot</code> Evaluation summary plot <code>ratio_of_samples_above_truth</code> Shows forecast bias across locations"},{"location":"chap-cli/evaluation-workflow.html#output-formats","title":"Output Formats","text":"<p>The output format is determined by file extension:</p> <ul> <li><code>.html</code> - Interactive HTML (recommended)</li> <li><code>.png</code> - Static PNG image</li> <li><code>.svg</code> - Vector SVG image</li> <li><code>.pdf</code> - PDF document</li> <li><code>.json</code> - Vega JSON specification</li> </ul>"},{"location":"chap-cli/evaluation-workflow.html#step-3-create-another-evaluation","title":"Step 3: Create Another Evaluation","text":"<p>Run the same process with a different model for comparison:</p> <pre><code>chap evaluate2 \\\n    --model-name https://github.com/dhis2-chap/chap_auto_ewars_weekly \\\n    --dataset-csv ./data/vietnam_data.csv \\\n    --output-file ./results/model_b_eval.nc \\\n    --backtest-params.n-periods 3 \\\n    --backtest-params.n-splits 7\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#step-4-export-and-compare-metrics","title":"Step 4: Export and Compare Metrics","text":"<p>Use <code>export-metrics</code> to compute metrics from multiple evaluations and export to CSV:</p> <pre><code>chap export-metrics \\\n    --input-files ./results/model_a_eval.nc ./results/model_b_eval.nc \\\n    --output-file ./results/comparison.csv\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#output-format","title":"Output Format","text":"<p>The CSV contains one row per evaluation with metadata and metric columns:</p> <pre><code>filename,model_name,model_version,rmse_aggregate,mae_aggregate,crps,ratio_within_10th_90th,ratio_within_25th_75th,test_sample_count\nmodel_a_eval.nc,minimalist_example_r,1.0.0,45.2,32.1,0.045,0.85,0.65,168\nmodel_b_eval.nc,chap_auto_ewars_weekly,2.0.0,38.7,28.4,0.038,0.88,0.70,168\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#available-metrics","title":"Available Metrics","text":"Metric ID Description <code>rmse_aggregate</code> Root Mean Squared Error (across all data) <code>mae_aggregate</code> Mean Absolute Error (across all data) <code>crps</code> Continuous Ranked Probability Score <code>ratio_within_10th_90th</code> Coverage ratio for 10th-90th percentile interval <code>ratio_within_25th_75th</code> Coverage ratio for 25th-75th percentile interval <code>test_sample_count</code> Number of test samples"},{"location":"chap-cli/evaluation-workflow.html#selecting-specific-metrics","title":"Selecting Specific Metrics","text":"<p>To export only specific metrics:</p> <pre><code>chap export-metrics \\\n    --input-files ./results/model_a_eval.nc ./results/model_b_eval.nc \\\n    --output-file ./results/comparison.csv \\\n    --metric-ids rmse_aggregate mae_aggregate crps\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#complete-example-standard-models","title":"Complete Example: Standard Models","text":"<p>Here's a complete workflow comparing two standard models using the included example dataset:</p> <pre><code># Step 1: Evaluate first model (auto-regressive)\nchap evaluate2 \\\n    --model-name https://github.com/dhis2-chap/chap_auto_ewars \\\n    --dataset-csv ./example_data/laos_subset.csv \\\n    --output-file ./eval_ewars.nc \\\n    --backtest-params.n-splits 3\n\n# Step 2: Plot first model results\nchap plot-backtest \\\n    --input-file ./eval_ewars.nc \\\n    --output-file ./plot_ewars.html\n\n# Step 3: Evaluate second model (minimalist R model)\nchap evaluate2 \\\n    --model-name https://github.com/dhis2-chap/minimalist_example_r \\\n    --dataset-csv ./example_data/laos_subset.csv \\\n    --output-file ./eval_minimalist.nc \\\n    --backtest-params.n-splits 3\n\n# Step 4: Plot second model results\nchap plot-backtest \\\n    --input-file ./eval_minimalist.nc \\\n    --output-file ./plot_minimalist.html\n\n# Step 5: Compare metrics\nchap export-metrics \\\n    --input-files ./eval_ewars.nc ./eval_minimalist.nc \\\n    --output-file ./model_comparison.csv\n\n# View the comparison\ncat ./model_comparison.csv\n</code></pre> <p>The GeoJSON file <code>example_data/laos_subset.geojson</code> is automatically discovered since it has the same base name as the CSV.</p>"},{"location":"chap-cli/evaluation-workflow.html#complete-example-chapkit-models","title":"Complete Example: Chapkit Models","text":"<p>Here's a workflow using chapkit models, including both a running service and a local directory:</p>"},{"location":"chap-cli/evaluation-workflow.html#option-a-using-a-running-chapkit-service","title":"Option A: Using a running chapkit service","text":"<p>First, start your chapkit model service (e.g., using Docker):</p> <pre><code>docker run -p 8000:8000 ghcr.io/dhis2-chap/chtorch:latest\n</code></pre> <p>Then run the evaluation:</p> <pre><code># Evaluate the chapkit model\nchap evaluate2 \\\n    --model-name http://localhost:8000 \\\n    --dataset-csv ./example_data/laos_subset.csv \\\n    --output-file ./eval_chapkit.nc \\\n    --run-config.is-chapkit-model \\\n    --backtest-params.n-splits 3\n\n# Plot results\nchap plot-backtest \\\n    --input-file ./eval_chapkit.nc \\\n    --output-file ./plot_chapkit.html\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#option-b-using-a-local-chapkit-model-directory-auto-start","title":"Option B: Using a local chapkit model directory (auto-start)","text":"<p>If you have a chapkit model in a local directory, CHAP can automatically start and stop the service:</p> <pre><code># Clone or create your chapkit model\ngit clone https://github.com/your-org/your-chapkit-model /path/to/chapkit-model\n\n# Evaluate with auto-start (CHAP starts the service automatically)\nchap evaluate2 \\\n    --model-name /path/to/chapkit-model \\\n    --dataset-csv ./example_data/laos_subset.csv \\\n    --output-file ./eval_local_chapkit.nc \\\n    --run-config.is-chapkit-model \\\n    --backtest-params.n-splits 3\n\n# Plot results\nchap plot-backtest \\\n    --input-file ./eval_local_chapkit.nc \\\n    --output-file ./plot_local_chapkit.html\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#comparing-chapkit-and-standard-models","title":"Comparing chapkit and standard models","text":"<p>You can compare chapkit models with standard models using export-metrics:</p> <pre><code># Evaluate a standard model\nchap evaluate2 \\\n    --model-name https://github.com/dhis2-chap/minimalist_example_r \\\n    --dataset-csv ./example_data/laos_subset.csv \\\n    --output-file ./eval_standard.nc \\\n    --backtest-params.n-splits 3\n\n# Evaluate a chapkit model\nchap evaluate2 \\\n    --model-name /path/to/chapkit-model \\\n    --dataset-csv ./example_data/laos_subset.csv \\\n    --output-file ./eval_chapkit.nc \\\n    --run-config.is-chapkit-model \\\n    --backtest-params.n-splits 3\n\n# Compare both\nchap export-metrics \\\n    --input-files ./eval_standard.nc ./eval_chapkit.nc \\\n    --output-file ./comparison.csv\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#tips","title":"Tips","text":"<ul> <li>Consistent parameters: Use the same <code>n-periods</code> and <code>n-splits</code> when comparing models</li> <li>Same dataset: Always use identical datasets for fair comparison</li> <li>Multiple runs: Consider running evaluations with different random seeds for robustness</li> <li>Metric interpretation: Lower RMSE/MAE/CRPS is better; higher coverage ratios indicate better calibrated uncertainty</li> <li>Chapkit auto-start: When using local chapkit directories, ensure <code>uv</code> is installed and the model directory has a valid FastAPI app structure with a <code>/health</code> endpoint</li> </ul>"},{"location":"contributor/index.html","title":"Contributor Guide","text":"<p>This section provides documentation for contributors to the Chap platform.</p>"},{"location":"contributor/index.html#getting-started","title":"Getting Started","text":"<ul> <li>Getting Started - Introduction for new contributors</li> <li>Setup Guide - Setting up your development environment</li> <li>Windows Setup - Windows-specific setup instructions</li> </ul>"},{"location":"contributor/index.html#understanding-the-codebase","title":"Understanding the Codebase","text":"<ul> <li>Code Overview - Architecture and code structure</li> <li>Vocabulary - Domain-specific terminology</li> </ul>"},{"location":"contributor/index.html#development","title":"Development","text":"<ul> <li>Testing - How to write and run tests</li> <li>Database Migrations - Managing database schema changes</li> <li>Documentation - Writing and building documentation</li> <li>Development Tools - Tools for development</li> </ul>"},{"location":"contributor/index.html#design-documents","title":"Design Documents","text":"<ul> <li>Evaluation Abstraction - Evaluation system design</li> <li>Preference Learning - Preference learning design</li> </ul>"},{"location":"contributor/chap-contributor-setup.html","title":"Setting Up CHAP Core as a Contributor","text":"<p>The following is our recommended setup for creating a development environment when working with CHAP Core as a contributor.</p> <p>If you are an external contributor without write-access to the chap-core repository you will first need to fork the chap-core repository to your own GitHub account.</p> <ol> <li> <p>Start by downloading the latest chap-core <code>master</code> branch to a local folder of your choice, either from the main repository or from your own fork:</p> <pre><code>$ git clone https://github.com/dhis2-chap/chap-core.git\n$ cd chap-core\n</code></pre> </li> <li> <p>If you need to work with and test a specific stable version of CHAP Core codebase, these are stored as version tags. Writing <code>git tag</code> on the commandline will give you a list of the available version. To switch to a desired version, for instance v1.0.3, you can write:</p> <pre><code>git switch tags/v1.0.3\n</code></pre> </li> <li> <p>If you're a Windows user, read this note about how to simulate a Linux environment using Windows WSL. Before proceeding to the next steps, initiate a wsl session from the commandline:</p> <pre><code>$ wsl\n</code></pre> </li> <li> <p>Install the uv package manager if you don't already have it. The benefit of using <code>uv</code> for the development environment is that it makes installing dependencies much faster.     To read more, check out their documentation.</p> <pre><code>* Fetch and install `uv` from their official website:\n\n  ```bash\n  $ curl -LsSf https://astral.sh/uv/install.sh | sh\n  ```\n\n* After the installation, restart the linux shell (or wsl if you're on windows) in order for the uv command to become available.\n</code></pre> </li> <li> <p>Install a local version of Python along with all the dependencies. Inside the project folder, run:</p> <pre><code>$ uv sync --dev\n</code></pre> <p>Note that <code>uv</code> creates a virtual Python environment with all the required packages for you, so you don\u2019t need to do this manually. This environment exists in the <code>.venv</code> directory.</p> </li> <li> <p>Activate the environment and run the tests to make sure everything is working:</p> <pre><code>$ source .venv/bin/activate\n$ pytest\n</code></pre> <p>We recommend a setup where you can run the tests directly through the IDE you are using (e.g. Vscode or Pycharm). Make sure that your IDE is using the correct Python environment.</p> </li> <li> <p>Finally, if the tests are passing, you should now be connected to the development version of Chap, directly reflecting     any changes you make to the code. Check to ensure that the chap command line interface (CLI) is available in your terminal:</p> <pre><code>  ```bash\n  $ chap --help\n  ```\n</code></pre> </li> </ol> <p>It is also a good to see if you can run chap evaluation on an external model, by running:</p> <pre><code>chap evaluate --model-name https://github.com/dhis2-chap/chap_auto_ewars --dataset-name ISIMIP_dengue_harmonized --dataset-country brazil\n</code></pre> <p>If the above command runs successfully, a report.pdf file will be generated with the results.</p> <p>You have now successfully setup a development version of the chap-cli tool and you are ready to start developing. If you have any problems installing or setting up the environment, feel free to contact us.</p>"},{"location":"contributor/code_overview.html","title":"Code overview","text":"<p>The following is a very brief overview of the main modules and parts of the chap-core code-base, which can be used as a starting point for getting to know the code:</p>"},{"location":"contributor/code_overview.html#the-chap-command-line-interface","title":"The chap command line interface","text":"<ul> <li>The entry point can be found in <code>cli.py</code>. Note that there is also a file called <code>chap_cli.py</code> which is an old entry point that is not being used.</li> <li>The <code>cli.py</code> file defines commands like <code>chap evaluate</code> are defined.</li> </ul> <p>By looking at the code in the <code>cli.py</code> file, you can see how the different commands are implemented, and follow the function calls to see what code is being used.</p>"},{"location":"contributor/code_overview.html#the-rest-api","title":"The REST API","text":"<p>The REST API is the main entry point for the Modeling App, and supports functionality like training models, predicting, harmonizing data etc.</p> <ul> <li>The main entry point for the API is in <code>rest_api/v1/rest_api.py</code> (newer versions will have a different version number than v1).</li> <li>The API is built using the <code>fastapi</code> library, and we are currently using Celery to handle asynchronous tasks (like training a model).</li> <li>Celery is currently abstracted away using the <code>CeleryPool</code> and <code>CeleryJob</code> classes.</li> </ul>"},{"location":"contributor/code_overview.html#more-about-the-rest-api-endpoints","title":"More about the rest API endpoints","text":"<p>The main endpoints for the REST API are defined in `rest_api/v[some version number]/rest_api.py.</p> <ul> <li>crud: mainly just database operations</li> <li>analytics: A bad name (should be changed in the future). Bigger things that are written specifically to be used by the frontend. Things that are not used by the modelling app should be deleted in the future (e.g. prediction-entry)</li> <li>debug:</li> <li>jobs:</li> <li>default: used by the old Prediction app (will be taken away at some point)</li> </ul> <p>We use pydantic models to define all input and return types in the REST API. See <code>rest_api/data_models.py</code>. We also use pydantic models to define database schemas (see <code>dataset_tables.py</code>). These models are overriden for the rest API if the REST API needs anything to be different. The database gives the objects IDs (if there is a primary key is default None). The overrides for the REST API have become a bit of mess and are defined many places. These should ideally be cleaned up and put in one file.</p> <p>A messy thing about the database models is that many tables have an id field that has the same behavious. This could ideally be solved by a decorator look for fields that have that behavious and create three classes from it: One for the database, one for Read and one for Create, so that we don't need to do inheritance to get these classes. This has to be done by adding methods to get the classes.</p>"},{"location":"contributor/code_overview.html#db-schemas","title":"DB schemas:","text":"<p>Everything that inherits from <code>SqlModel</code> AND has <code>table=True</code> becomes a database table. The easiest way to find tables is to simply search for <code>table=True</code></p>"},{"location":"contributor/code_overview.html#external-models","title":"External models","text":"<p>The codebase contains various abtractions for external models. The general idea is that an external model is defined by what commands it uses to train and predict, and what kind of environment (e.g. docker) it needs to run these commands. CHAP then handles the necessary steps to call these commands in the given environment with correct data files.</p>"},{"location":"contributor/code_overview.html#runners","title":"Runners","text":"<p>The <code>TrainPredictRunner</code> class defines an interface that provides method for running commands for training and predicting for a given model. The <code>DockerTrainPredictRunner</code> class is a concrete implementation that defines how to run train/predict-commands in a docker environment.</p>"},{"location":"contributor/code_overview.html#external-model-wrapping","title":"External model wrapping","text":"<p>The <code>ExternalModel</code> class is used to represent an external model, and contains the necessary information for running the mode, like the runner (an object of a subclass of <code>TrainPredictRunner</code>, the model name etc).</p> <p>This class is rarely used directly. The easiest way to parse a model specification and get an object of <code>ExternalModel</code> is to use the <code>get_model_from_directory_or_github_url</code> function. This function can take a directory or a github url, and parses the model specification in order to get an <code>ExternalModel</code> object with a suitable runner. By following the code in this function, you can see how external models are loaded and run.</p>"},{"location":"contributor/code_overview.html#model-evaluation-and-testtrain-splitting","title":"Model evaluation and test/train splitting","text":"<p>A big nontrivial part of chap is to correctly split data into train and test sets for evaluation and passing these to models for evaluation.</p> <p>A good starting point for understanding this process is the <code>evaluate_model</code> in the <code>prediction_evaluator.py</code> file. Functions like the <code>train_test_generator</code> function are relevant. Currently, the main evaluation flow does not compute metrics, but simply plots the predictions and the actual values (in the <code>plot_forecasts</code> function).</p>"},{"location":"contributor/code_overview.html#models-and-modeltemplates","title":"Models and ModelTemplates","text":"<p>The following is a draft mermaid notation overview:</p> <pre><code>flowchart TD\n\n\n    ModelTemplate_get_config_class --&gt; ModelConfiguration\n\n\n\n     E[evaluate or predict or backtest]--&gt; get_model_template_from_directory_or_github_url --&gt;\n    get_model_template_from_mlproject_file --&gt; ModelTemplate\n\n    ModelTemplate --&gt; A[get_model with object of ModelConfiguratio] --&gt; Model\n\n    ModelTemplate --&gt; B[\"get_default_model()\"] --&gt; Model\n    ModelTemplate --&gt; get_train_predict_runner --&gt; TrainPredictRunner\n\n\n    deprecated --&gt; get_model_from_directory_or_github_url --&gt; get_model_from_mlproject_file\n\n\n\n\n    Runner --&gt; CommandLineRunner\n\n    TrainPredictRunner --&gt; DockerTrainPredictRunner\n    TrainPredictRunner --&gt; CommandLineTrainPredictRunner --&gt; CommandLineRunner\n</code></pre>"},{"location":"contributor/database_migrations.html","title":"Database Migrations with Alembic","text":"<p>CHAP uses Alembic to manage database schema changes. Migrations run automatically when you start the application with <code>docker compose up</code>.</p>"},{"location":"contributor/database_migrations.html#prerequisites","title":"Prerequisites","text":"<p>Make sure the database is running:</p> <pre><code>docker compose -f compose.yml -f compose.dev.yml up -d postgres\n</code></pre> <p>The <code>compose.dev.yml</code> file exposes the postgres port on localhost for development, allowing you to run Alembic commands locally and connect with database tools like pgAdmin or DBeaver.</p> <p>Alembic will use the default database URL (<code>postgresql://root:thisisnotgoingtobeexposed@localhost:5432/chap_core</code>) when running locally. If you need to connect to a different database, set the <code>CHAP_DATABASE_URL</code> environment variable:</p> <pre><code>export CHAP_DATABASE_URL=\"postgresql://user:password@host:port/database\"\n</code></pre>"},{"location":"contributor/database_migrations.html#quick-start-making-schema-changes","title":"Quick Start: Making Schema Changes","text":""},{"location":"contributor/database_migrations.html#1-modify-the-database-model","title":"1. Modify the Database Model","text":"<p>Edit the SQLModel class in <code>chap_core/database/</code>:</p> <pre><code># chap_core/database/dataset_tables.py\nclass DataSet(DataSetBase, table=True):\n    # Add a new field\n    new_field: Optional[str] = None\n</code></pre>"},{"location":"contributor/database_migrations.html#2-generate-a-migration","title":"2. Generate a Migration","text":"<pre><code>uv run alembic revision --autogenerate -m \"add_new_field_to_dataset\"\n</code></pre> <p>This creates a new file in <code>alembic/versions/</code> with the detected schema changes.</p>"},{"location":"contributor/database_migrations.html#3-review-the-migration","title":"3. Review the Migration","text":"<p>Open the generated file and verify the <code>upgrade()</code> and <code>downgrade()</code> functions are correct:</p> <pre><code>def upgrade() -&gt; None:\n    op.add_column('dataset', sa.Column('new_field', sa.String(), nullable=True))\n\ndef downgrade() -&gt; None:\n    op.drop_column('dataset', 'new_field')\n</code></pre>"},{"location":"contributor/database_migrations.html#4-test-locally","title":"4. Test Locally","text":"<pre><code># Apply the migration\nuv run alembic upgrade head\n\n# Check current version\nuv run alembic current\n\n# If needed, rollback\nuv run alembic downgrade -1\n</code></pre>"},{"location":"contributor/database_migrations.html#5-commit-and-deploy","title":"5. Commit and Deploy","text":"<pre><code>git add alembic/versions/*.py\ngit commit -m \"Add new_field to dataset table\"\n</code></pre> <p>When deployed, the migration runs automatically on startup.</p>"},{"location":"contributor/database_migrations.html#common-commands","title":"Common Commands","text":"<pre><code># Show current migration version\nuv run alembic current\n\n# Show migration history\nuv run alembic history\n\n# Apply all pending migrations\nuv run alembic upgrade head\n\n# Rollback one migration\nuv run alembic downgrade -1\n\n# Create empty migration (for data changes)\nuv run alembic revision -m \"migrate_old_data\"\n</code></pre>"},{"location":"contributor/database_migrations.html#how-migrations-work","title":"How Migrations Work","text":"<p>When you run <code>docker compose up</code>, the system runs migrations in this order:</p> <ol> <li>Custom migrations - For backward compatibility with older database versions (v1.0.17 and earlier)</li> <li>Table creation - Creates any new tables</li> <li>Alembic migrations - Applies schema changes from <code>alembic/versions/</code></li> </ol> <p>This hybrid approach ensures smooth upgrades from older versions while using a standard migration tool for future changes.</p> <p>Note: The custom migration system in <code>chap_core/database/database.py</code> (steps 1 and 2) is a temporary solution for backward compatibility. Once all production instances have upgraded past v1.0.17, we can remove <code>_run_generic_migration()</code> and <code>_run_v1_0_17_migrations()</code> in a future release, leaving only Alembic for all schema management.</p>"},{"location":"contributor/database_migrations.html#best-practices","title":"Best Practices","text":"<ul> <li>Always review autogenerated migrations before committing</li> <li>Test migrations locally before pushing</li> <li>Write descriptive messages using imperative mood (e.g., \"add_column\" not \"added_column\")</li> <li>Don't modify committed migrations - create a new one instead</li> <li>Test rollback to ensure <code>downgrade()</code> works</li> </ul>"},{"location":"contributor/database_migrations.html#data-migrations","title":"Data Migrations","text":"<p>For changes that require updating existing data (not just schema):</p> <pre><code># Create empty migration\nuv run alembic revision -m \"migrate_user_data\"\n</code></pre> <p>Edit the file to add custom SQL or Python logic:</p> <pre><code>def upgrade() -&gt; None:\n    # Update existing data\n    op.execute(\"UPDATE users SET status = 'active' WHERE created &gt; '2024-01-01'\")\n\ndef downgrade() -&gt; None:\n    # Reverse if possible\n    op.execute(\"UPDATE users SET status = NULL WHERE created &gt; '2024-01-01'\")\n</code></pre>"},{"location":"contributor/database_migrations.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributor/database_migrations.html#autogenerate-detects-unwanted-changes","title":"Autogenerate detects unwanted changes","text":"<p>This usually means the database schema doesn't match your models. Either: - The database has old columns that should be removed - Your model types don't match the database types</p> <p>Review the migration and adjust as needed.</p>"},{"location":"contributor/database_migrations.html#migration-fails-with-revision-not-found","title":"Migration fails with \"revision not found\"","text":"<p>Your local <code>alembic/versions/</code> is out of sync with the database:</p> <pre><code># Pull latest migrations\ngit pull\n\n# Try again\nuv run alembic upgrade head\n</code></pre>"},{"location":"contributor/database_migrations.html#learn-more","title":"Learn More","text":"<ul> <li>Full documentation: <code>alembic/README.md</code></li> <li>Alembic documentation</li> <li>SQLModel documentation</li> </ul>"},{"location":"contributor/development_tools.html","title":"Introduction to Development Tools","text":"<p>To get started with development, it\u2019s helpful to be familiar with some key tools:</p> <ul> <li>Version Control (GitHub): Track changes to your code and collaborate with others. A good starting point is the GitHub Guides.  </li> <li>Containerisation (Docker): Package applications with all their dependencies to run reliably across environments. Learn the basics at the Docker Getting Started page.  </li> <li>Integrated Development Environments (IDEs): Tools like PyCharm, VS Code, or Eclipse help you write, test, and debug code efficiently. See VS Code Documentation or PyCharm Guide.  </li> <li>Python and Package Management: Install Python and manage project dependencies with <code>pip</code> or <code>conda</code>. See Python.org and pip documentation.  </li> <li>Virtual Environments: Isolate project dependencies to avoid conflicts. Learn more at the Python venv docs.</li> </ul>"},{"location":"contributor/evaluation_abstraction.html","title":"Evaluation Abstraction Design","text":""},{"location":"contributor/evaluation_abstraction.html#overview","title":"Overview","text":"<p>This document describes the proposed refactoring to create a database-agnostic Evaluation abstraction. The goal is to unify how evaluations/backtests are represented throughout the codebase, enabling better code reuse between the REST API and CLI evaluation workflows.</p>"},{"location":"contributor/evaluation_abstraction.html#problem-statement","title":"Problem Statement","text":"<p>Currently, the codebase has two different approaches to handling model evaluations:</p> <ol> <li>REST API: Uses the <code>BackTest</code> database model (tied to SQLModel/database)</li> <li>CLI: Uses GluonTS Evaluator directly without database persistence</li> </ol> <p>This duplication leads to: - Code that cannot be easily shared between REST API and CLI - Different evaluation workflows that are hard to maintain - Tight coupling between evaluation logic and database schema</p>"},{"location":"contributor/evaluation_abstraction.html#current-state-analysis","title":"Current State Analysis","text":""},{"location":"contributor/evaluation_abstraction.html#database-model-structure","title":"Database Model Structure","text":"<p>The current BackTest implementation is defined in <code>chap_core/database/tables.py:39-47</code>:</p> <pre><code>class BackTest(_BackTestRead, table=True):\n    id: Optional[int] = Field(primary_key=True, default=None)\n    dataset: DataSet = Relationship()\n    forecasts: List[\"BackTestForecast\"] = Relationship(back_populates=\"backtest\", cascade_delete=True)\n    metrics: List[\"BackTestMetric\"] = Relationship(back_populates=\"backtest\", cascade_delete=True)\n    aggregate_metrics: Dict[str, float] = Field(default_factory=dict, sa_column=Column(JSON))\n    model_db_id: int = Field(foreign_key=\"configuredmodeldb.id\")\n    configured_model: Optional[\"ConfiguredModelDB\"] = Relationship()\n</code></pre> <p>Key components:</p> <ol> <li>BackTestForecast (<code>tables.py:113-118</code>): Stores individual forecast predictions</li> <li>Fields: <code>period</code>, <code>org_unit</code>, <code>values</code> (samples), <code>last_train_period</code>, <code>last_seen_period</code></li> <li> <p>One record per location-period-split combination</p> </li> <li> <p>BackTestMetric (<code>tables.py:121-137</code>): Deprecated, not used in new metric system</p> </li> <li> <p>Related metadata:</p> </li> <li><code>org_units: List[str]</code> - evaluated locations</li> <li><code>split_periods: List[PeriodID]</code> - train/test split points</li> <li><code>model_db_id</code> - reference to configured model</li> <li><code>dataset</code> - relationship to DataSet table</li> </ol>"},{"location":"contributor/evaluation_abstraction.html#rest-api-workflow","title":"REST API Workflow","text":"<p>Location: <code>chap_core/rest_api/v1/routers/analytics.py</code> and <code>chap_core/rest_api/db_worker_functions.py</code></p> <p>Evaluation Creation Process:</p> <pre><code>1. POST /create-backtest\n   \u2514\u2500&gt; Queue worker: run_backtest()\n       \u2514\u2500&gt; Load dataset and configured model\n       \u2514\u2500&gt; Call _backtest() -&gt; returns Iterable[DataSet[SamplesWithTruth]]\n       \u2514\u2500&gt; session.add_evaluation_results() -&gt; persists to BackTest table\n       \u2514\u2500&gt; Returns backtest.id\n</code></pre> <p>Data Consumption:</p> <ol> <li>GET /evaluation-entry (<code>analytics.py:217-284</code>):</li> <li>Queries BackTestForecast records</li> <li>Returns quantiles for specified split_period and org_units</li> <li> <p>Can aggregate to \"adm0\" level</p> </li> <li> <p>Metric Computation (<code>assessment/metrics/__init__.py:84-102</code>):    <pre><code>def compute_all_aggregated_metrics_from_backtest(backtest: BackTest):\n    flat_forecasts = convert_backtest_to_flat_forecasts(backtest.forecasts)\n    flat_observations = convert_backtest_observations_to_flat_observations(\n        backtest.dataset.observations\n    )\n    # Compute metrics using flat representations\n    for metric in metrics:\n        result = metric.get_metric(flat_observations, flat_forecasts)\n</code></pre></p> </li> <li> <p>Visualization (<code>plotting/evaluation_plot.py:236-243</code>):    <pre><code>def make_plot_from_backtest_object(backtest: BackTest, plotting_class, metric):\n    flat_forecasts = convert_backtest_to_flat_forecasts(backtest.forecasts)\n    flat_observations = convert_backtest_observations_to_flat_observations(\n        backtest.dataset.observations\n    )\n    metric_data = metric.compute(flat_observations, flat_forecasts)\n    return plotting_class(metric_data).plot_spec()\n</code></pre></p> </li> </ol> <p>Key Pattern: BackTest DB object \u2192 Flat DataFrame representation \u2192 Metrics/Visualization</p>"},{"location":"contributor/evaluation_abstraction.html#cli-workflow","title":"CLI Workflow","text":"<p>Location: <code>chap_core/cli.py:189-309</code> and <code>chap_core/assessment/prediction_evaluator.py:58-118</code></p> <p>Evaluation Process:</p> <pre><code>1. cli.py evaluate command\n   \u2514\u2500&gt; Load model template and get configured model\n   \u2514\u2500&gt; Call evaluate_model(estimator, data, ...)\n       \u2514\u2500&gt; Uses train_test_generator() for data splits\n       \u2514\u2500&gt; estimator.train() and predictor.predict()\n       \u2514\u2500&gt; Uses GluonTS Evaluator directly\n       \u2514\u2500&gt; Returns (aggregate_metrics, item_metrics) tuple\n   \u2514\u2500&gt; Save results to CSV files\n   \u2514\u2500&gt; No database persistence\n</code></pre> <p>Key Differences from REST API: - No BackTest database model used - Results stay in memory as Python dicts/tuples - Direct use of GluonTS evaluation - CSV export instead of database storage</p>"},{"location":"contributor/evaluation_abstraction.html#flat-representation-system","title":"Flat Representation System","text":"<p>Location: <code>chap_core/assessment/flat_representations.py</code></p> <p>The codebase has a well-established flat representation system for working with evaluation data:</p> <p>FlatForecasts: Tabular format for forecasts <pre><code>Columns: location, time_period, horizon_distance, sample, forecast\nExample:\nlocation  | time_period | horizon_distance | sample | forecast\n----------|-------------|------------------|--------|----------\nregion_A  | 2024-01     | 1                | 0      | 45.2\nregion_A  | 2024-01     | 1                | 1      | 48.7\nregion_A  | 2024-02     | 2                | 0      | 52.1\n...\n</code></pre></p> <p>FlatObserved: Tabular format for observations <pre><code>Columns: location, time_period, disease_cases\nExample:\nlocation  | time_period | disease_cases\n----------|-------------|---------------\nregion_A  | 2024-01     | 47.0\nregion_A  | 2024-02     | 51.5\n...\n</code></pre></p> <p>Conversion Functions:</p> <ol> <li><code>convert_backtest_to_flat_forecasts(backtest_forecasts: List[BackTestForecast])</code>:</li> <li>Converts BackTestForecast records to FlatForecasts DataFrame</li> <li>Calculates <code>horizon_distance</code> from period differences</li> <li> <p>Unpacks sample arrays into individual rows</p> </li> <li> <p><code>convert_backtest_observations_to_flat_observations(observations: List[ObservationBase])</code>:</p> </li> <li>Extracts disease_cases observations</li> <li>Returns FlatObserved DataFrame</li> </ol> <p>Usage: All metrics and visualization code works with flat representations, not database models directly.</p>"},{"location":"contributor/evaluation_abstraction.html#sampleswithtruth-intermediate-format","title":"SamplesWithTruth Intermediate Format","text":"<p>Location: <code>chap_core/datatypes.py:361</code></p> <p>During evaluation, results are generated as <code>DataSet[SamplesWithTruth]</code>:</p> <pre><code>@tsdataclass\nclass SamplesWithTruth(Samples):\n    disease_cases: float  # truth value\n    # Inherited from Samples:\n    # time_period: TimePeriod\n    # samples: np.ndarray  # forecast samples\n</code></pre> <p>This is the in-memory format returned by <code>_backtest()</code> and then persisted to database via <code>add_evaluation_results()</code>.</p>"},{"location":"contributor/evaluation_abstraction.html#current-data-flow","title":"Current Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         REST API Path                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                               \u2502\n\u2502  _backtest()                                                  \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  Iterable[DataSet[SamplesWithTruth]]                         \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  session.add_evaluation_results()                            \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  BackTest (DB) \u2190 \u2500\u2500 \u2500\u2500 stored in database                    \u2502\n\u2502      \u251c\u2500&gt; BackTestForecast records                            \u2502\n\u2502      \u2514\u2500&gt; DataSet relationship                                \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  convert_backtest_to_flat_*()                                \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  FlatForecasts + FlatObserved DataFrames                     \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  Metrics / Visualization                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          CLI Path                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                               \u2502\n\u2502  evaluate_model()                                             \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  GluonTS Evaluator                                            \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  (aggregate_metrics, item_metrics) tuples                    \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  Save to CSV                                                  \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  No database persistence                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#proposed-design","title":"Proposed Design","text":""},{"location":"contributor/evaluation_abstraction.html#core-concept-flatevaluationdata","title":"Core Concept: FlatEvaluationData","text":"<p>First, we define a simple dataclass that combines forecasts and observations together:</p> <pre><code>from dataclasses import dataclass\nfrom chap_core.assessment.flat_representations import FlatForecasts, FlatObserved\n\n@dataclass\nclass FlatEvaluationData:\n    \"\"\"\n    Container for flat representations of evaluation data.\n\n    Combines forecasts and observations which are always used together\n    for metric computation and visualization.\n    \"\"\"\n    forecasts: FlatForecasts\n    observations: FlatObserved\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#core-concept-evaluationbase-abc","title":"Core Concept: EvaluationBase ABC","text":"<p>Create an abstract base class that defines the interface for all evaluation representations, decoupled from database implementation:</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import List\n\nclass EvaluationBase(ABC):\n    \"\"\"\n    Abstract base class for evaluation results.\n\n    An Evaluation represents the complete results of evaluating a model:\n    - Forecasts (with samples/quantiles)\n    - Observations (ground truth)\n    - Metadata (locations, split periods)\n\n    This abstraction is database-agnostic and can be implemented by\n    different concrete classes (database-backed, in-memory, etc.).\n    \"\"\"\n\n    @abstractmethod\n    def to_flat(self) -&gt; FlatEvaluationData:\n        \"\"\"\n        Export evaluation data as flat representations.\n\n        Returns:\n            FlatEvaluationData containing FlatForecasts and FlatObserved objects\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_org_units(self) -&gt; List[str]:\n        \"\"\"\n        Get list of locations included in this evaluation.\n\n        Returns:\n            List of location identifiers (org_units)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_split_periods(self) -&gt; List[str]:\n        \"\"\"\n        Get list of train/test split periods used in evaluation.\n\n        Returns:\n            List of period identifiers (e.g., [\"2024-01\", \"2024-02\"])\n        \"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def from_backtest(cls, backtest: \"BackTest\") -&gt; \"EvaluationBase\":\n        \"\"\"\n        Create Evaluation from database BackTest object.\n\n        All implementations must support loading from database.\n\n        Args:\n            backtest: Database BackTest object (with relationships loaded)\n\n        Returns:\n            Evaluation instance\n        \"\"\"\n        pass\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#concrete-implementation-evaluation","title":"Concrete Implementation: Evaluation","text":"<p>Wraps existing BackTest database model to implement the abstract interface:</p> <pre><code>class Evaluation(EvaluationBase):\n    \"\"\"\n    Evaluation implementation backed by database BackTest model.\n\n    This wraps an existing BackTest object and provides the\n    EvaluationBase interface without modifying the database schema.\n    \"\"\"\n\n    def __init__(self, backtest: BackTest):\n        \"\"\"\n        Args:\n            backtest: Database BackTest object\n        \"\"\"\n        self._backtest = backtest\n        self._flat_data_cache = None\n\n    @classmethod\n    def from_backtest(cls, backtest: BackTest) -&gt; \"Evaluation\":\n        \"\"\"\n        Create Evaluation from database BackTest object.\n\n        Args:\n            backtest: Database BackTest object (with relationships loaded)\n\n        Returns:\n            Evaluation instance\n        \"\"\"\n        return cls(backtest)\n\n    def to_backtest(self) -&gt; BackTest:\n        \"\"\"\n        Get underlying database BackTest object.\n\n        Returns:\n            BackTest database model\n        \"\"\"\n        return self._backtest\n\n    def to_flat(self) -&gt; FlatEvaluationData:\n        \"\"\"Export evaluation data using existing conversion functions.\"\"\"\n        if self._flat_data_cache is None:\n            from chap_core.assessment.flat_representations import (\n                FlatForecasts,\n                FlatObserved,\n                convert_backtest_to_flat_forecasts,\n                convert_backtest_observations_to_flat_observations,\n            )\n\n            forecasts_df = convert_backtest_to_flat_forecasts(\n                self._backtest.forecasts\n            )\n            observations_df = convert_backtest_observations_to_flat_observations(\n                self._backtest.dataset.observations\n            )\n\n            self._flat_data_cache = FlatEvaluationData(\n                forecasts=FlatForecasts(forecasts_df),\n                observations=FlatObserved(observations_df),\n            )\n        return self._flat_data_cache\n\n    def get_org_units(self) -&gt; List[str]:\n        \"\"\"Get locations from BackTest metadata.\"\"\"\n        return self._backtest.org_units\n\n    def get_split_periods(self) -&gt; List[str]:\n        \"\"\"Get split periods from BackTest metadata.\"\"\"\n        return self._backtest.split_periods\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#future-implementation-inmemoryevaluation","title":"Future Implementation: InMemoryEvaluation","text":"<p>For CLI and other non-database use cases:</p> <pre><code>class InMemoryEvaluation(EvaluationBase):\n    \"\"\"\n    Evaluation implementation using in-memory data structures.\n\n    Suitable for CLI workflows where database persistence is not needed.\n    Can be created directly from evaluation results or flat DataFrames.\n    \"\"\"\n\n    def __init__(\n        self,\n        flat_data: FlatEvaluationData,\n        org_units: List[str],\n        split_periods: List[str],\n    ):\n        \"\"\"\n        Args:\n            flat_data: FlatEvaluationData containing forecasts and observations\n            org_units: List of location identifiers\n            split_periods: List of split period identifiers\n        \"\"\"\n        self._flat_data = flat_data\n        self._org_units = org_units\n        self._split_periods = split_periods\n\n    @classmethod\n    def from_backtest(cls, backtest: BackTest) -&gt; \"InMemoryEvaluation\":\n        \"\"\"\n        Create InMemoryEvaluation from database BackTest object.\n\n        Converts database representation to in-memory format.\n\n        Args:\n            backtest: Database BackTest object (with relationships loaded)\n\n        Returns:\n            InMemoryEvaluation instance\n        \"\"\"\n        from chap_core.assessment.flat_representations import (\n            FlatForecasts,\n            FlatObserved,\n            convert_backtest_to_flat_forecasts,\n            convert_backtest_observations_to_flat_observations,\n        )\n\n        forecasts_df = convert_backtest_to_flat_forecasts(backtest.forecasts)\n        observations_df = convert_backtest_observations_to_flat_observations(\n            backtest.dataset.observations\n        )\n\n        flat_data = FlatEvaluationData(\n            forecasts=FlatForecasts(forecasts_df),\n            observations=FlatObserved(observations_df),\n        )\n\n        return cls(\n            flat_data=flat_data,\n            org_units=backtest.org_units,\n            split_periods=backtest.split_periods,\n        )\n\n    @classmethod\n    def from_samples_with_truth(\n        cls,\n        results: Iterable[DataSet[SamplesWithTruth]],\n        last_train_period: TimePeriod,\n    ) -&gt; \"InMemoryEvaluation\":\n        \"\"\"\n        Create from _backtest() results without database persistence.\n\n        Args:\n            results: Iterator of DataSet[SamplesWithTruth] from backtest\n            last_train_period: Final training period\n\n        Returns:\n            InMemoryEvaluation instance\n        \"\"\"\n        # Convert SamplesWithTruth to flat representations\n        # (implementation details omitted for brevity)\n        pass\n\n    def to_flat(self) -&gt; FlatEvaluationData:\n        \"\"\"Return flat data directly.\"\"\"\n        return self._flat_data\n\n    def get_org_units(self) -&gt; List[str]:\n        \"\"\"Return stored org_units.\"\"\"\n        return self._org_units\n\n    def get_split_periods(self) -&gt; List[str]:\n        \"\"\"Return stored split_periods.\"\"\"\n        return self._split_periods\n\n    def to_backtest(self, session: SessionWrapper, info: BackTestCreate) -&gt; BackTest:\n        \"\"\"\n        Persist to database as BackTest.\n\n        Args:\n            session: Database session wrapper\n            info: Metadata for creating BackTest record\n\n        Returns:\n            Persisted BackTest object\n        \"\"\"\n        # Convert flat representations to BackTest structure\n        # (implementation details omitted for brevity)\n        pass\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#api-usage-examples","title":"API Usage Examples","text":""},{"location":"contributor/evaluation_abstraction.html#example-1-rest-api-loading-and-computing-metrics","title":"Example 1: REST API - Loading and Computing Metrics","text":"<pre><code># Current approach (tightly coupled to database)\nbacktest = session.get_backtest(backtest_id)\nflat_forecasts = convert_backtest_to_flat_forecasts(backtest.forecasts)\nflat_observations = convert_backtest_observations_to_flat_observations(\n    backtest.dataset.observations\n)\n# Compute metrics manually\nfor metric in [RMSE(), MAE(), CRPS()]:\n    metric_df = metric.get_metric(flat_observations, flat_forecasts)\n\n# Proposed approach (using abstraction)\nbacktest_db = session.get_backtest(backtest_id)\nevaluation = Evaluation.from_backtest(backtest_db)\n\n# Get flat data for metric computation\nflat_data = evaluation.to_flat()\nfor metric in [RMSE(), MAE(), CRPS()]:\n    metric_df = metric.get_metric(flat_data.observations, flat_data.forecasts)\n\n# Access metadata\norg_units = evaluation.get_org_units()\nsplit_periods = evaluation.get_split_periods()\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#example-2-visualization","title":"Example 2: Visualization","text":"<pre><code># Current approach\ndef make_plot_from_backtest_object(backtest: BackTest, metric):\n    flat_forecasts = convert_backtest_to_flat_forecasts(backtest.forecasts)\n    flat_observations = convert_backtest_observations_to_flat_observations(\n        backtest.dataset.observations\n    )\n    metric_data = metric.compute(flat_observations, flat_forecasts)\n    return plot(metric_data)\n\n# Proposed approach (works with any EvaluationBase implementation)\ndef make_plot_from_evaluation(evaluation: EvaluationBase, metric):\n    flat_data = evaluation.to_flat()\n    metric_data = metric.compute(flat_data.observations, flat_data.forecasts)\n    return plot(metric_data)\n\n# Usage\nevaluation = Evaluation.from_backtest(backtest_db)\nchart = make_plot_from_evaluation(evaluation, RMSE())\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#example-3-cli-evaluation-future","title":"Example 3: CLI Evaluation (Future)","text":"<pre><code># Current CLI approach\ndef evaluate(data, model_name, ...):\n    estimator = load_model(model_name)\n    aggregate_metrics, item_metrics = evaluate_model(estimator, data)\n    save_to_csv(aggregate_metrics, \"results.csv\")\n\n# Proposed approach with InMemoryEvaluation\ndef evaluate(data, model_name, ...):\n    estimator = load_model(model_name)\n    results = _backtest(estimator, data)\n\n    # Create in-memory evaluation (no database)\n    evaluation = InMemoryEvaluation.from_samples_with_truth(\n        results, last_train_period\n    )\n\n    # Get flat data\n    flat_data = evaluation.to_flat()\n\n    # Use same metric computation as REST API\n    for metric in [RMSE(), MAE(), CRPS()]:\n        metric_df = metric.get_metric(flat_data.observations, flat_data.forecasts)\n        # Process metric_df...\n\n    # Export to CSV (accessing underlying DataFrames)\n    flat_data.forecasts.to_csv(\"forecasts.csv\")\n    flat_data.observations.to_csv(\"observations.csv\")\n\n    # Optionally persist to database\n    if persist:\n        backtest_db = evaluation.to_backtest(session, backtest_info)\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#example-4-comparing-evaluations","title":"Example 4: Comparing Evaluations","text":"<pre><code>def compare_evaluations(eval1: EvaluationBase, eval2: EvaluationBase):\n    \"\"\"\n    Compare two evaluations regardless of their underlying implementation.\n    Works with Evaluation, InMemoryEvaluation, or any future implementation.\n    \"\"\"\n    # Check compatibility\n    assert eval1.get_org_units() == eval2.get_org_units()\n    assert eval1.get_split_periods() == eval2.get_split_periods()\n\n    # Get flat data for both\n    flat_data1 = eval1.to_flat()\n    flat_data2 = eval2.to_flat()\n\n    # Compute same metrics for both\n    results1 = {}\n    results2 = {}\n    for metric in [RMSE(), CRPS()]:\n        results1[metric.spec.metric_id] = metric.get_metric(\n            flat_data1.observations, flat_data1.forecasts\n        )\n        results2[metric.spec.metric_id] = metric.get_metric(\n            flat_data2.observations, flat_data2.forecasts\n        )\n\n    # Compare results (implementation varies by metric output format)\n    return results1, results2\n\n# Usage works with any combination\n# Both implementations support from_backtest()\neval1 = Evaluation.from_backtest(session.get_backtest(1))\neval2 = InMemoryEvaluation.from_backtest(session.get_backtest(2))\nresults1, results2 = compare_evaluations(eval1, eval2)\n\n# Or use from_samples_with_truth() for CLI results\neval_from_cli = InMemoryEvaluation.from_samples_with_truth(results, ...)\nresults_cli, results_db = compare_evaluations(eval_from_cli, eval1)\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  FlatEvaluationData (dataclass)                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  + forecasts: FlatForecasts                                    \u2502\n\u2502  + observations: FlatObserved                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u25b3\n                               \u2502 returns\n                               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      EvaluationBase (ABC)                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  + to_flat() -&gt; FlatEvaluationData                             \u2502\n\u2502  + get_org_units() -&gt; List[str]                                \u2502\n\u2502  + get_split_periods() -&gt; List[str]                            \u2502\n\u2502  + from_backtest(backtest) -&gt; EvaluationBase [classmethod]     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u25b3\n                               \u2502 implements\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Evaluation         \u2502  \u2502   InMemoryEvaluation          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  - _backtest: BackTest       \u2502  \u2502  - _flat_data:                \u2502\n\u2502  - _flat_data_cache          \u2502  \u2502      FlatEvaluationData       \u2502\n\u2502                              \u2502  \u2502  - _org_units: List[str]      \u2502\n\u2502                              \u2502  \u2502  - _split_periods: List[str]  \u2502\n\u2502  + from_backtest()           \u2502  \u2502                               \u2502\n\u2502  + to_backtest()             \u2502  \u2502  + from_samples_with_truth()  \u2502\n\u2502  + to_flat()                 \u2502  \u2502  + to_backtest()              \u2502\n\u2502  + get_org_units()           \u2502  \u2502  + to_flat()                  \u2502\n\u2502  + get_split_periods()       \u2502  \u2502  + get_org_units()            \u2502\n\u2502                              \u2502  \u2502  + get_split_periods()        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                   \u2502\n         \u2502 wraps                             \u2502 stores\n         \u25bc                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BackTest (DB Model)         \u2502  \u2502  FlatEvaluationData           \u2502\n\u2502  + forecasts: List[...]      \u2502  \u2502  (in-memory)                  \u2502\n\u2502  + dataset: DataSet          \u2502  \u2502                               \u2502\n\u2502  + org_units: List[str]      \u2502  \u2502                               \u2502\n\u2502  + split_periods: List[str]  \u2502  \u2502                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#benefits-of-this-approach","title":"Benefits of This Approach","text":"<ol> <li> <p>Code Reuse: Metric computation, visualization, and analysis code can work with any EvaluationBase implementation</p> </li> <li> <p>Database Decoupling: Core evaluation logic no longer depends on database schema</p> </li> <li> <p>Flexibility: Easy to add new implementations (e.g., for different storage backends, remote APIs, etc.)</p> </li> <li> <p>Migration Path: Can introduce gradually without breaking existing code:</p> </li> <li>Start with Evaluation wrapping existing BackTest</li> <li>Update visualization/metrics to accept EvaluationBase</li> <li>Later add InMemoryEvaluation for CLI</li> <li> <p>Eventually unify REST API and CLI workflows</p> </li> <li> <p>Testing: Easier to test evaluation logic without database setup</p> </li> <li> <p>Caching: Implementations can cache expensive conversions (flat representations)</p> </li> </ol>"},{"location":"contributor/evaluation_abstraction.html#implementation-strategy","title":"Implementation Strategy","text":"<p>The implementation will be done in phases to minimize risk and allow for incremental progress. Phase 1 is the immediate next step - implementing the Evaluation classes without changing any existing code.</p>"},{"location":"contributor/evaluation_abstraction.html#implementation-phases","title":"Implementation Phases","text":""},{"location":"contributor/evaluation_abstraction.html#phase-0-design-current-phase","title":"Phase 0: Design (Current Phase)","text":"<p>Goal: Document the design and get team alignment</p> <p>Tasks: - Create design document (this document) - Review and discuss with team - Get alignment on approach - Refine design based on feedback</p> <p>Deliverable: Approved design document</p>"},{"location":"contributor/evaluation_abstraction.html#phase-1-core-implementation-first-step-keep-it-simple","title":"Phase 1: Core Implementation (First Step - Keep It Simple)","text":"<p>Goal: Implement the Evaluation abstraction without changing any existing code</p> <p>Scope: Create new classes only - no refactoring of existing code</p> <p>New File: <code>chap_core/assessment/evaluation.py</code></p> <p>Classes to Implement:</p> <ol> <li> <p><code>FlatEvaluationData</code> (dataclass):    <pre><code>@dataclass\nclass FlatEvaluationData:\n    forecasts: FlatForecasts\n    observations: FlatObserved\n</code></pre></p> </li> <li> <p><code>EvaluationBase</code> (ABC):</p> </li> <li>Abstract method: <code>to_flat() -&gt; FlatEvaluationData</code></li> <li>Abstract method: <code>get_org_units() -&gt; List[str]</code></li> <li>Abstract method: <code>get_split_periods() -&gt; List[str]</code></li> <li> <p>Abstract classmethod: <code>from_backtest(backtest) -&gt; EvaluationBase</code></p> </li> <li> <p><code>Evaluation</code> (concrete implementation):</p> </li> <li>Constructor: <code>__init__(self, backtest: BackTest)</code></li> <li>Classmethod: <code>from_backtest(backtest) -&gt; Evaluation</code></li> <li>Method: <code>to_backtest() -&gt; BackTest</code> (return wrapped object)</li> <li>Method: <code>to_flat() -&gt; FlatEvaluationData</code> (with caching)</li> <li>Method: <code>get_org_units() -&gt; List[str]</code></li> <li>Method: <code>get_split_periods() -&gt; List[str]</code></li> </ol> <p>Testing: - Create <code>tests/test_evaluation.py</code> - Test <code>Evaluation.from_backtest()</code> with mock BackTest - Test <code>to_flat()</code> returns correct types and data - Test metadata accessors work correctly - Verify conversion matches existing <code>convert_backtest_to_flat_*()</code> functions</p> <p>What we do NOT do in Phase 1: - \u274c Change any existing REST API code - \u274c Change any existing CLI code - \u274c Change any visualization or metric computation code - \u274c Change the database schema - \u274c Implement InMemoryEvaluation (that's Phase 3)</p> <p>Success Criteria: - All tests pass - Can create <code>Evaluation</code> from database <code>BackTest</code> object - Can convert to flat representations correctly - Code is documented with docstrings - No existing code is modified</p> <p>Deliverable: New <code>evaluation.py</code> module with working, tested classes that can load from database but aren't yet used anywhere</p>"},{"location":"contributor/evaluation_abstraction.html#phase-2-rest-api-integration","title":"Phase 2: REST API Integration","text":"<p>Goal: Refactor REST API to use the Evaluation abstraction</p> <p>Tasks: 1. Update analytics router endpoints to work with <code>EvaluationBase</code> 2. Update worker functions to optionally return <code>Evaluation</code> 3. Update metric computation functions to accept <code>EvaluationBase</code> 4. Update visualization functions to accept <code>EvaluationBase</code> 5. Ensure backward compatibility throughout 6. Add integration tests</p> <p>Files to modify: - <code>chap_core/rest_api/v1/routers/analytics.py</code> - <code>chap_core/rest_api/db_worker_functions.py</code> - <code>chap_core/assessment/metrics/__init__.py</code> - <code>chap_core/plotting/evaluation_plot.py</code></p> <p>Deliverable: REST API using Evaluation abstraction while maintaining all existing functionality</p>"},{"location":"contributor/evaluation_abstraction.html#phase-3-cli-integration","title":"Phase 3: CLI Integration","text":"<p>Goal: Implement InMemoryEvaluation and refactor CLI to use it</p> <p>Tasks: 1. Implement <code>InMemoryEvaluation</code> class:    - Implements <code>from_backtest()</code> for loading from DB    - Implements <code>from_samples_with_truth()</code> for CLI workflow    - Implements <code>to_backtest()</code> for optional persistence 2. Refactor <code>cli.py</code> evaluate command to use <code>InMemoryEvaluation</code> 3. Share metric computation code between REST API and CLI 4. Add CLI-specific tests</p> <p>Deliverable: CLI and REST API using same evaluation abstraction and metric computation</p>"},{"location":"contributor/evaluation_abstraction.html#phase-4-code-consolidation","title":"Phase 4: Code Consolidation","text":"<p>Goal: Remove duplication and clean up deprecated code</p> <p>Tasks: 1. Identify and remove duplicated evaluation logic 2. Consolidate metric computation into shared utilities 3. Update documentation and examples 4. Remove deprecated functions if any 5. Performance optimization if needed</p> <p>Deliverable: Cleaner codebase with less duplication and better maintainability</p>"},{"location":"contributor/evaluation_abstraction.html#open-questions-for-discussion","title":"Open Questions for Discussion","text":"<ol> <li>Naming: Should we use \"Evaluation\" or keep \"Backtest\" terminology?</li> <li>Evaluation is more general and not database-specific</li> <li> <p>Backtest is already established in codebase</p> </li> <li> <p>Aggregate Metrics: Should EvaluationBase include <code>get_aggregate_metrics()</code>?</p> </li> <li>Pro: Matches BackTest schema which has <code>aggregate_metrics</code> field</li> <li> <p>Con: Metrics should be computed on-demand, not stored in evaluation</p> </li> <li> <p>Model Information: Should Evaluation include model configuration?</p> </li> <li>Currently BackTest has <code>model_db_id</code> and <code>configured_model</code> relationship</li> <li> <p>For database-agnostic design, should this be optional metadata?</p> </li> <li> <p>Serialization: Should we add methods like <code>to_json()</code>, <code>from_json()</code>?</p> </li> <li>Useful for API responses and caching</li> <li> <p>May be better as separate utility functions</p> </li> <li> <p>Performance: Should we optimize for lazy loading or eager loading?</p> </li> <li>Evaluation caches flat representations</li> <li>InMemoryEvaluation stores them directly</li> <li>Should conversion be done on-demand or upfront?</li> </ol>"},{"location":"contributor/evaluation_abstraction.html#related-files","title":"Related Files","text":"<p>Key files that would be affected by this refactoring:</p> <ul> <li><code>chap_core/database/tables.py</code> - BackTest model (unchanged, wrapped by Evaluation)</li> <li><code>chap_core/assessment/flat_representations.py</code> - Conversion functions (reused by implementations)</li> <li><code>chap_core/assessment/metrics/</code> - Metric computation (updated to accept EvaluationBase)</li> <li><code>chap_core/plotting/evaluation_plot.py</code> - Visualization (updated to accept EvaluationBase)</li> <li><code>chap_core/rest_api/v1/routers/analytics.py</code> - REST API endpoints (gradually migrated)</li> <li><code>chap_core/cli.py</code> - CLI evaluate command (future integration with InMemoryEvaluation)</li> </ul>"},{"location":"contributor/evaluation_abstraction.html#conclusion","title":"Conclusion","text":"<p>The proposed EvaluationBase abstraction provides a clean separation between evaluation data and storage implementation. By starting with Evaluation as a wrapper, we can introduce this pattern gradually without breaking changes, then progressively refactor to achieve better code reuse between REST API and CLI workflows.</p> <p>The key insight is that most evaluation-related code only needs access to flat representations and metadata, not the full database model structure. By defining this interface explicitly, we make dependencies clear and enable more flexible implementations.</p>"},{"location":"contributor/getting_started.html","title":"Contributor getting started","text":"<p>The main intended way of contributing to CHAP-Core is by contributing with models, for which we have a modularized system that makes it easy to contribute. For this, we have guides/tutorials that explain how to make models compatible with CHAP.</p> <p>We are also working on adding similar guides for contributing with custom code for evaluating models and visualizing results. The code for evaluating and visualizing results is currently tightly integrated into the chap-core code base, but the plan is to make this more modularized and easier to contribute to.</p> <p>This document describes how to get started for contributing to the chap-core code base itself.</p>"},{"location":"contributor/getting_started.html#getting-started-working-with-the-chap-core-codebase","title":"Getting started working with the chap-core codebase","text":"<p>If you're new to CHAP Core, it can be useful to see the code overview guide for a brief overview of the code base.</p>"},{"location":"contributor/getting_started.html#windows-users","title":"Windows users","text":"<p>Windows users who wish to contribute to CHAP Core should start by reading this important note.</p>"},{"location":"contributor/getting_started.html#development-setup","title":"Development setup","text":"<p>In order to make changes and contribute back to the chap-core Python codebase, you will need to set up a development environment.</p> <p>Installing and activating the development environment above is a required step for the remaining steps below.</p>"},{"location":"contributor/getting_started.html#local-installation-of-a-dhis2-instance-with-the-dhis2-modeling-app","title":"Local installation of a DHIS2 instance with the DHIS2 Modeling app","text":"<p>If you want to test chap-core with the Modeling app, follow these steps to set up a local installation of DHIS2.</p> <p>We have an internal database that can be used to set up a DHIS2 instance with testdata. If you are an internal developer, you will have access to this through our internal drive. Follow these steps (if you don't have access to this database, and want to set up a general instance, see steps below):</p> <ul> <li>Download the zip-file, unzip it and run <code>docker compose up</code> in the unzipped directory.</li> <li>Note: If you are on linux, you will have to edit the docker-compose.yaml file and change <code>platform</code> to <code>linux/amd64</code>.</li> <li>Note: You may have to restart the web docker container if this started before the db container was up.</li> <li>Run analytics by opening Data administration, go to analytics tables, uncheck all boxes and click \"Start export\"</li> </ul> <p>To set up a DHIS2 instance without this test db, do the following:</p> <ul> <li>Follow these instructions to install the DHIS2 cli tools</li> <li>Spin up a DHIS2 instance by running <code>d2 cluster up 2.41 --db-version 2.41</code> (More details here). Change the version number with whatever version you want.</li> </ul> <p>After following any of the guides above, you should have a DHIS2 instance running at localhost:8080.</p> <ul> <li>Go to that url in your webbrowser and log in.</li> <li>First install the <code>App Management</code> app, then install the app called <code>Modeling</code> through the App Hub.</li> <li>In the Modeling app, you will be told to put in an url to Chap. Since DHIS2 runs through a Docker container, you will need to put in an IP to your local computer. This ip can be found by running <code>ifconfig | grep \"inet \" | grep -v 127.0.0.1 | awk '{print $2}'</code> in your terminal (you may have to install ifconfig). Put <code>http://</code> before that IP and <code>:8000/**</code> after, e.g. <code>http://172.18.0.1:8000/**</code>.</li> </ul>"},{"location":"contributor/getting_started.html#code-guidelines","title":"Code guidelines","text":"<p>In the current phase we are moving quite fast, and the code guidelines are not very strict. However, we have some general guidelines that we try to follow:</p> <ul> <li>Alle code that is meant to be used should be tested (see the guidelines about testing)</li> <li>It is okay to have code that is not currently being used (just write a comment to explain)</li> </ul>"},{"location":"contributor/getting_started.html#debugging","title":"Debugging","text":"<p>Debugging can be done as usual in your favorite code editor.</p> <p>For Windows users using VSCode, since the code should be run and tested on WSL, follow these steps to enable debugging in VSCode:</p> <ul> <li>Install the WSL extension for WSL.</li> <li>Inside a wsl commandline session in your chap-core folder, type <code>code .</code></li> <li>This will open your chap-core folder in VSCode using the WSL Linux/Python development environment. You can now use the VSCode debugger as usual.</li> </ul>"},{"location":"contributor/getting_started.html#testing","title":"Testing","text":"<p>The CHAP Core codebase relies heavily on testing to ensure that the code works properly. A quick example to run a specific test file would be to write:</p> <pre><code>$ pytest tests/test_polygons.py\n</code></pre> <p>See more about our guidelines for testing in the testing guide.</p>"},{"location":"contributor/getting_started.html#code-formatting","title":"Code formatting","text":"<p>To ensure consistent and standardized code formatting we recommend running the <code>ruff</code> tool available from the development environment before making commits which will automatically check and report any formatting issues:</p> <pre><code>$ ruff check\n</code></pre>"},{"location":"contributor/getting_started.html#docstring-style-guide","title":"Docstring style guide","text":"<p>All docstrings should follow the NumPy style guide for consistency and clarity.</p> <p>Ensure that function and class docstrings include appropriate sections such as 'Parameters' and 'Returns'.</p>"},{"location":"contributor/getting_started.html#documentation","title":"Documentation","text":"<p>Changes to the CHAP Core documentation is done inside the <code>docs</code> folder, and can be built by writing:</p> <pre><code>$ cd docs\n$ make html\n</code></pre> <p>More detailed guidelines for how to write and build the documentation can be found here.</p>"},{"location":"contributor/getting_started.html#contributing-code","title":"Contributing code","text":"<p>Code contributions should always be made to the <code>dev</code> branch first. When the <code>dev</code> branch has been used and tested for some time, the CHAP team will merge this into the <code>master</code> branch.</p> <p>Before making your contribution, always run the quick test suite to make sure everything works.</p> <p>Most of the time, contributions should be made on a new branch, and creating a Pull Request targeting the <code>dev</code> branch of the chap-core repository.</p> <p>If you're an internal developer and only making small changes it's sometimes fine to push directly to the <code>dev</code> branch. However, for major changes or code refactoring, internal developers should still consider creating and submitting a PR for more systematic review of the code.</p>"},{"location":"contributor/preference_learning.html","title":"Preference Learning","text":"<p>This document explains the preference learning system and how to implement a custom <code>PreferenceLearner</code> algorithm.</p>"},{"location":"contributor/preference_learning.html#overview","title":"Overview","text":"<p>Preference learning is a technique for discovering optimal model configurations through iterative A/B testing. Instead of using automated optimization metrics alone, it can incorporate human judgment to select preferred models based on visual inspection of backtest plots.</p> <p>The system works by: 1. Generating candidate model configurations from a hyperparameter search space 2. Running backtests on pairs of candidates 3. Presenting results to a decision maker (human or automated) 4. Learning from preferences to propose better candidates 5. Repeating until convergence or max iterations</p>"},{"location":"contributor/preference_learning.html#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     preference_learn CLI                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502  1. Load dataset and search space                               \u2502\n\u2502                                                                  \u2502\n\u2502  2. Initialize PreferenceLearner                                \u2502\n\u2502     \u2514\u2500\u2500 PreferenceLearner.init(model_name, search_space)        \u2502\n\u2502                                                                  \u2502\n\u2502  3. Main loop:                                                  \u2502\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502     \u2502 candidates = learner.get_next_candidates()          \u2502     \u2502\n\u2502     \u2502                                                      \u2502     \u2502\n\u2502     \u2502 for each candidate:                                  \u2502     \u2502\n\u2502     \u2502   \u2514\u2500\u2500 Run backtest \u2192 Evaluation                     \u2502     \u2502\n\u2502     \u2502                                                      \u2502     \u2502\n\u2502     \u2502 metrics = compute_metrics(evaluations)              \u2502     \u2502\n\u2502     \u2502                                                      \u2502     \u2502\n\u2502     \u2502 preferred_idx = decision_maker.decide(evaluations)  \u2502     \u2502\n\u2502     \u2502                                                      \u2502     \u2502\n\u2502     \u2502 learner.report_preference(candidates,               \u2502     \u2502\n\u2502     \u2502                           preferred_idx, metrics)   \u2502     \u2502\n\u2502     \u2502                                                      \u2502     \u2502\n\u2502     \u2502 learner.save(state_file)                            \u2502     \u2502\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                                                  \u2502\n\u2502  4. best = learner.get_best_candidate()                         \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributor/preference_learning.html#key-components","title":"Key Components","text":""},{"location":"contributor/preference_learning.html#modelcandidate","title":"ModelCandidate","text":"<p>Represents a model configuration to evaluate:</p> <pre><code>from chap_core.preference_learning.preference_learner import ModelCandidate\n\ncandidate = ModelCandidate(\n    model_name=\"my_model\",\n    configuration={\"learning_rate\": 0.01, \"hidden_size\": 128}\n)\n</code></pre>"},{"location":"contributor/preference_learning.html#comparisonresult","title":"ComparisonResult","text":"<p>Records the result of comparing candidates:</p> <pre><code>from chap_core.preference_learning.preference_learner import ComparisonResult\n\nresult = ComparisonResult(\n    candidates=[candidate_a, candidate_b],\n    preferred_index=1,  # candidate_b was preferred\n    metrics=[{\"mae\": 0.8}, {\"mae\": 0.5}],\n    iteration=0\n)\n\n# Access the winner\nwinner = result.preferred  # Returns candidate_b\n</code></pre>"},{"location":"contributor/preference_learning.html#search-space","title":"Search Space","text":"<p>The search space defines hyperparameters to explore. It uses parsed values from <code>load_search_space_from_config()</code>:</p> <pre><code>from chap_core.hpo.base import load_search_space_from_config, Int, Float\n\n# Raw YAML config format:\nraw_config = {\n    \"learning_rate\": {\"low\": 0.001, \"high\": 0.1, \"type\": \"float\", \"log\": True},\n    \"hidden_size\": {\"low\": 32, \"high\": 256, \"type\": \"int\"},\n    \"optimizer\": {\"values\": [\"adam\", \"sgd\", \"rmsprop\"]}\n}\n\n# Parsed search space contains Int, Float, or list objects:\nsearch_space = load_search_space_from_config(raw_config)\n# Result: {\n#     \"learning_rate\": Float(low=0.001, high=0.1, log=True),\n#     \"hidden_size\": Int(low=32, high=256),\n#     \"optimizer\": [\"adam\", \"sgd\", \"rmsprop\"]\n# }\n</code></pre>"},{"location":"contributor/preference_learning.html#implementing-a-preferencelearner","title":"Implementing a PreferenceLearner","text":"<p>To implement a custom preference learning algorithm, subclass <code>PreferenceLearnerBase</code>:</p> <pre><code>from pathlib import Path\nfrom typing import Any, Optional\nfrom chap_core.preference_learning.preference_learner import (\n    PreferenceLearnerBase,\n    ModelCandidate,\n    ComparisonResult,\n    SearchSpaceValue,\n)\n\n\nclass MyPreferenceLearner(PreferenceLearnerBase):\n    \"\"\"\n    Custom preference learning algorithm.\n\n    This could implement:\n    - Bayesian optimization with preference feedback\n    - Multi-armed bandit approaches\n    - Genetic algorithms\n    - etc.\n    \"\"\"\n\n    def __init__(self, state: MyLearnerState):\n        \"\"\"Initialize with internal state.\"\"\"\n        self._state = state\n\n    @classmethod\n    def init(\n        cls,\n        model_name: str,\n        search_space: dict[str, SearchSpaceValue],\n        max_iterations: int = 10,\n    ) -&gt; \"MyPreferenceLearner\":\n        \"\"\"\n        Initialize a new learner with a parsed search space.\n\n        Args:\n            model_name: Name of the model template to optimize\n            search_space: Parsed search space dict mapping param names to\n                         Int, Float, or list of categorical values.\n            max_iterations: Maximum number of comparison iterations\n\n        Returns:\n            New MyPreferenceLearner instance\n        \"\"\"\n        # Generate initial candidates from search space\n        candidates = cls._generate_candidates(model_name, search_space)\n\n        state = MyLearnerState(\n            model_name=model_name,\n            candidates=candidates,\n            max_iterations=max_iterations,\n        )\n        return cls(state)\n\n    def save(self, filepath: Path) -&gt; None:\n        \"\"\"\n        Save learner state to a file.\n\n        This enables resuming learning across sessions.\n        \"\"\"\n        import json\n        with open(filepath, \"w\") as f:\n            json.dump(self._state.to_dict(), f, indent=2)\n\n    @classmethod\n    def load(cls, filepath: Path) -&gt; \"MyPreferenceLearner\":\n        \"\"\"Load learner from a saved state file.\"\"\"\n        import json\n        with open(filepath, \"r\") as f:\n            data = json.load(f)\n        state = MyLearnerState.from_dict(data)\n        return cls(state)\n\n    def get_next_candidates(self) -&gt; Optional[list[ModelCandidate]]:\n        \"\"\"\n        Get the next set of candidates to compare.\n\n        Returns:\n            List of 2+ ModelCandidates to compare, or None if done.\n\n        This is where your algorithm's intelligence lives:\n        - Which candidates should be compared next?\n        - How do you balance exploration vs exploitation?\n        - How do you use past preferences to guide selection?\n        \"\"\"\n        if self.is_complete():\n            return None\n\n        # Your algorithm logic here\n        # Example: select candidates based on uncertainty, expected improvement, etc.\n        return [self._state.candidates[0], self._state.candidates[1]]\n\n    def report_preference(\n        self,\n        candidates: list[ModelCandidate],\n        preferred_index: int,\n        metrics: list[dict],\n    ) -&gt; None:\n        \"\"\"\n        Report the result of a comparison.\n\n        Args:\n            candidates: The candidates that were compared\n            preferred_index: Index of the preferred candidate (0-based)\n            metrics: Computed metrics for each candidate\n\n        Use this feedback to update your internal model:\n        - Update belief distributions\n        - Eliminate inferior candidates\n        - Adjust exploration strategy\n        \"\"\"\n        result = ComparisonResult(\n            candidates=candidates,\n            preferred_index=preferred_index,\n            metrics=metrics,\n            iteration=self._state.current_iteration,\n        )\n        self._state.comparison_history.append(result)\n        self._state.current_iteration += 1\n        self._state.best_candidate = candidates[preferred_index]\n\n    def is_complete(self) -&gt; bool:\n        \"\"\"Check if learning is complete.\"\"\"\n        return self._state.current_iteration &gt;= self._state.max_iterations\n\n    def get_best_candidate(self) -&gt; Optional[ModelCandidate]:\n        \"\"\"Get the current best candidate.\"\"\"\n        return self._state.best_candidate\n\n    def get_comparison_history(self) -&gt; list[ComparisonResult]:\n        \"\"\"Get all comparison results.\"\"\"\n        return self._state.comparison_history\n\n    @property\n    def current_iteration(self) -&gt; int:\n        \"\"\"Get current iteration number.\"\"\"\n        return self._state.current_iteration\n</code></pre>"},{"location":"contributor/preference_learning.html#preferencelearnerbase-interface","title":"PreferenceLearnerBase Interface","text":"<p>The abstract base class defines these required methods:</p> Method Description <code>init(model_name, search_space, max_iterations)</code> Class method to create a new learner <code>save(filepath)</code> Save state to file for persistence <code>load(filepath)</code> Class method to restore from saved state <code>get_next_candidates()</code> Return next candidates to compare, or None if done <code>report_preference(candidates, preferred_index, metrics)</code> Record comparison result <code>is_complete()</code> Check if learning should stop <code>get_best_candidate()</code> Return current best candidate <code>get_comparison_history()</code> Return all comparison results <code>current_iteration</code> Property returning current iteration number"},{"location":"contributor/preference_learning.html#existing-implementation-tournamentpreferencelearner","title":"Existing Implementation: TournamentPreferenceLearner","text":"<p>The default implementation uses a tournament-style bracket:</p> <ol> <li>Initialization: Generates all candidate configurations from the search space</li> <li>Selection: Winners advance to compete against other winners or uncompared candidates</li> <li>Termination: Stops after max_iterations or when no more pairs to compare</li> </ol> <p>This is a simple but effective baseline. More sophisticated algorithms could: - Use Bayesian optimization to model the preference function - Implement Thompson sampling for exploration - Use Elo ratings to rank candidates - Apply genetic algorithms with preference-based selection</p>"},{"location":"contributor/preference_learning.html#decisionmaker","title":"DecisionMaker","text":"<p>The <code>DecisionMaker</code> determines which candidate is preferred:</p> <pre><code>from chap_core.preference_learning.decision_maker import (\n    DecisionMaker,\n    VisualDecisionMaker,\n    MetricDecisionMaker,\n)\n\n# Visual: Opens plots in browser, user chooses\nvisual_dm = VisualDecisionMaker()\n\n# Metric: Automatic selection based on computed metrics\nmetric_dm = MetricDecisionMaker(\n    metrics=[{\"mae\": 0.8}, {\"mae\": 0.5}],\n    metric_names=[\"mae\", \"rmse\"],  # Priority order\n    lower_is_better=True\n)\n\n# Use it\npreferred_idx = decision_maker.decide(evaluations)\n</code></pre>"},{"location":"contributor/preference_learning.html#cli-usage","title":"CLI Usage","text":"<pre><code>chap preference-learn \\\n    --model-name ../my_model \\\n    --dataset-csv ./data.csv \\\n    --search-space-yaml ./search_space.yaml \\\n    --state-file ./preference_state.json \\\n    --decision-mode metric \\\n    --decision-metrics mae rmse \\\n    --max-iterations 10\n</code></pre>"},{"location":"contributor/preference_learning.html#testing-your-implementation","title":"Testing Your Implementation","text":"<pre><code>import pytest\nfrom pathlib import Path\n\n\nclass TestMyPreferenceLearner:\n    def test_init_creates_candidates(self):\n        search_space = {\"param\": [1, 2, 3]}\n        learner = MyPreferenceLearner.init(\n            model_name=\"test\",\n            search_space=search_space,\n            max_iterations=5,\n        )\n        assert learner.current_iteration == 0\n        assert not learner.is_complete()\n\n    def test_get_next_candidates_returns_pair(self):\n        learner = MyPreferenceLearner.init(\n            model_name=\"test\",\n            search_space={\"x\": [1, 2]},\n        )\n        candidates = learner.get_next_candidates()\n        assert candidates is not None\n        assert len(candidates) == 2\n\n    def test_report_preference_updates_state(self):\n        learner = MyPreferenceLearner.init(\n            model_name=\"test\",\n            search_space={\"x\": [1, 2]},\n        )\n        candidates = learner.get_next_candidates()\n        learner.report_preference(\n            candidates=candidates,\n            preferred_index=0,\n            metrics=[{\"mae\": 0.5}, {\"mae\": 0.7}],\n        )\n        assert learner.current_iteration == 1\n        assert learner.get_best_candidate() == candidates[0]\n\n    def test_save_and_load_roundtrip(self, tmp_path):\n        learner = MyPreferenceLearner.init(\n            model_name=\"test\",\n            search_space={\"x\": [1, 2]},\n        )\n        candidates = learner.get_next_candidates()\n        learner.report_preference(candidates, 0, [{\"mae\": 0.5}, {\"mae\": 0.7}])\n\n        state_file = tmp_path / \"state.json\"\n        learner.save(state_file)\n\n        loaded = MyPreferenceLearner.load(state_file)\n        assert loaded.current_iteration == 1\n        assert len(loaded.get_comparison_history()) == 1\n</code></pre>"},{"location":"contributor/preference_learning.html#example-search-space","title":"Example Search Space","text":"<p>See <code>example_data/preference_learning/ewars_hpo_search_space.yaml</code> for a complete example:</p> <pre><code># Number of lags to include in the model (integer parameter)\nn_lags:\n  low: 1\n  high: 6\n  type: int\n\n# Prior on precision of fixed effects - acts as regularization\n# Using log scale since this spans multiple orders of magnitude\nprecision:\n  low: 0.001\n  high: 1.0\n  type: float\n  log: true\n</code></pre>"},{"location":"contributor/preference_learning.html#file-locations","title":"File Locations","text":"<ul> <li><code>chap_core/preference_learning/preference_learner.py</code> - Base class and TournamentPreferenceLearner</li> <li><code>chap_core/preference_learning/decision_maker.py</code> - DecisionMaker implementations</li> <li><code>chap_core/cli_endpoints/preference_learn.py</code> - CLI endpoint</li> <li><code>tests/preference_learning/</code> - Tests</li> <li><code>example_data/preference_learning/</code> - Example search space files</li> </ul>"},{"location":"contributor/testing.html","title":"Testing while developing","text":"<p>We rely on having most of the codebase well tested, so that we can be confident that new changes don't break stuff. Although there is some overhead writing tests, having good tests makes developing and pushing new features much faster.</p> <p>We use pytest as our testing framework. To run the tests.</p> <p>The tests are split into quick tests that one typically runs often while developing and more comprehensive tests that are run less frequently.</p> <p>We recomment the following:</p> <ul> <li>Run the quick tests frequently while developing. Ideally have a shortcut or easy way to run these through your IDE.</li> <li>Run the comprehensive tests before pushing new code. These are also run automatically on Github actions, but we want to try to avoid these failing there, so we try to discover problems ideally before pushing new code.</li> </ul>"},{"location":"contributor/testing.html#the-quick-tests","title":"The quick tests","text":"<p>First make sure you have activated your local development environment:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>The quick test can be run simply by running <code>pytest</code> in the root folder of the project:</p> <pre><code>pytest\n</code></pre> <p>All tests should pass. If you write a new test and it is not passing for some reason (e.g. the functionalit you are testing is not implemented yet), you can mark the test as <code>xfail</code> by adding the <code>@pytest.mark.xfail</code> decorator to the test function. This will make the test not fail the test suite.</p> <pre><code>import pytest\n\n@pytest.mark.xfail\ndef test_my_function():\n    assert False\n</code></pre> <p>If you have slow tests that you don't want to be included every time you run pytest, you can mark them as slow.</p> <pre><code>import pytest\n\n@pytest.mark.slow\ndef test_my_slow_function():\n    assert True\n</code></pre> <p>Such tests are not included when running pytest, but included when running <code>pytest --run-slow</code> (see below).</p>"},{"location":"contributor/testing.html#the-comprehensive-tests","title":"The comprehensive tests","text":"<p>The comprehensive tests include the quick tests (see above) in addition to:</p> <ul> <li>slow tests (marked with <code>@pytest.mark.slow</code>). </li> <li>Some tests for the integration with various docker containers </li> <li>Pytest run on all files in the scripts directory that contains <code>_example</code> in the file name. The idea is that one can put code examples here that are then automatically tested.</li> <li>Docetests (all tests and code in the documentation)</li> </ul> <p>The comprehensive tests are run by running this in the root folder of the project:</p> <pre><code>make test-all\n</code></pre> <p>To see what is actually being run, you can see what is specified under <code>test-all</code> in the Makefile.</p>"},{"location":"contributor/testing.html#some-more-details-about-integration-tests","title":"Some more details about integration tests","text":"<ul> <li>The file <code>docker_db_flow.py</code> is important: This runs a lot of the db integration tests and tests for endpoints that are using the database and is run through a docker image when <code>make test-all</code> is run. Similarily the docker_flow.py runs some of the old endpoints (the db_flow.py is outdated and don't need to be run in future).</li> <li>Everything that the frontend uses should currently be added as tests in the docker_db_flow.py file</li> <li>In the future, we should ideally use the pytest framework for this integration test</li> </ul>"},{"location":"contributor/testing.html#documentation-testing","title":"Documentation testing","text":"<p>We automatically test code blocks in our documentation to ensure examples stay up-to-date and work correctly. This uses mktestdocs to extract and execute code blocks from markdown files.</p>"},{"location":"contributor/testing.html#two-tier-testing-system","title":"Two-tier testing system","text":"<p>Documentation tests are split into two tiers:</p> Tier Command When to run Duration Fast <code>make test-docs</code> Every PR, pre-commit ~10 seconds Slow <code>make test-docs-slow</code> Weekly CI, manual ~20 seconds <p>Fast tests validate Python and bash code blocks in markdown files. They run automatically as part of the regular test suite.</p> <p>Slow tests validate: - JSON/YAML data format examples (schema validation) - CLI help command output - Python import statements from documentation - Example dataset existence</p>"},{"location":"contributor/testing.html#running-documentation-tests","title":"Running documentation tests","text":"<pre><code># Run fast documentation tests\nmake test-docs\n\n# Run slow documentation tests (requires --run-slow flag)\nmake test-docs-slow\n\n# Run all documentation tests\nmake test-docs-all\n</code></pre>"},{"location":"contributor/testing.html#writing-testable-code-blocks","title":"Writing testable code blocks","text":"<p>When adding code examples to documentation, use the appropriate language tag:</p> Tag Tested Use for <code>python</code> Yes Python code that can be executed <code>bash</code> Yes Shell commands that can be executed safely <code>console</code> No Commands requiring external resources (docker, network, etc.) <code>text</code> No Plain text output examples <code>json</code> No JSON data examples (validated in slow tests via fixtures) <code>yaml</code> No YAML configuration examples <p>Example - Testable bash command:</p> <pre><code>```bash\nchap --help\n```\n</code></pre> <p>Example - Non-testable command (uses <code>console</code>):</p> <pre><code>```console\ndocker run -p 8000:8000 ghcr.io/dhis2-chap/chtorch:latest\n```\n</code></pre>"},{"location":"contributor/testing.html#skipping-files","title":"Skipping files","text":"<p>Some documentation files contain examples that cannot be safely tested (require external models, docker, network access, etc.). These are listed in <code>tests/test_documentation.py</code> in the <code>SKIP_FILES</code> list.</p> <p>If you add a new documentation file that cannot be tested, add it to <code>SKIP_FILES</code> with a comment explaining why.</p>"},{"location":"contributor/testing.html#adding-slow-test-fixtures","title":"Adding slow test fixtures","text":"<p>If you add new JSON/YAML examples to documentation that should be validated, add corresponding fixtures in <code>tests/fixtures/doc_test_data.py</code> and tests in <code>tests/test_documentation_slow.py</code>.</p> <p>For example, to validate a new JSON schema:</p> <pre><code># In tests/fixtures/doc_test_data.py\nMY_NEW_DATA_FORMAT = {\n    \"required_fields\": [\"field1\", \"field2\"],\n    \"example\": {\"field1\": \"value1\", \"field2\": \"value2\"}\n}\n\n# In tests/test_documentation_slow.py\n@pytest.mark.slow\ndef test_my_new_data_format():\n    assert \"field1\" in MY_NEW_DATA_FORMAT[\"example\"]\n</code></pre>"},{"location":"contributor/vocabulary.html","title":"Vocabulary (domain specific terms used in the code)","text":""},{"location":"contributor/vocabulary.html#model-template","title":"Model template","text":"<p>A model template is a flexible \"model\" which can be configured. A model template typically presents various options (hyperparameters, possible covariates, etc) which are open for configuration. </p>"},{"location":"contributor/vocabulary.html#configured-model","title":"Configured model","text":"<p>A configured model can be made from a model template by applying chocies to the options presented by the model template. Only a configured model can actually be trained on a given dataset (as opposed to a model template, since a model template does not necessarily have enough information about how to train or predict).</p>"},{"location":"contributor/vocabulary.html#externalmodel","title":"ExternalModel","text":"<p>ExternalModel is a wrapper around an external model (that can e.g. be an R model) to make it compatible with the interface of ConfiguredModel. This means that ExternalModel has train/predict similarily to ConfiguredModel, but these methods are wrappers that runs the train/predict of external models.</p>"},{"location":"contributor/vocabulary.html#some-other-terms-we-use","title":"Some other terms we use","text":"<p>Backtest: Is the same as evaluation for now (used as a term in the REST API)</p>"},{"location":"contributor/vocabulary.html#runner","title":"Runner","text":"<p>A runner is something that can run commands. ExternalModels (not ConfiguredModels) have a Runner object attached to them. When train/predict is called, the runner is handling how to j6o05..,m.</p>"},{"location":"contributor/vocabulary.html#backtest-and-evaluation","title":"Backtest and Evaluation","text":"<p>Backtest and evaluation are used interchangeably for now. They refer to the process of evaluating a model on a dataset by splitting the dataset into training and test sets multiple times (e.g. using cross-validation) and measuring performance on the test sets.</p> <p>Evaluations is a common term in the machine learning domain. Backtest is often used more specifically in time series forecasting, where the model is tested on past data to see how well it would have performed.</p>"},{"location":"contributor/windows_contributors.html","title":"Important note for Windows contributors","text":"<p>Due to the limited support for Windows in many of the dependencies and to ensure a consistent development environment,  Windows users should always use wsl to operate in a Linux environment. </p>"},{"location":"contributor/windows_contributors.html#first-time-wsl-setup","title":"First time WSL setup","text":"<p>If this is the first time you're using wsl on Windows:</p> <ul> <li> <p>First create a wsl linux environment with <code>wsl install</code></p> </li> <li> <p>Make docker available from within the wsl environment:</p> </li> <li> <p>In Docker Desktop, go to Settings - Resources - WSL Integration and check off the Linux distro used by wsl, e.g. <code>Ubuntu</code></p> </li> </ul>"},{"location":"contributor/windows_contributors.html#running-commands-through-wsl","title":"Running commands through WSL","text":"<p>If you're a Windows contributor, always remember to first enter the linux environment before you run any commands CHAP CLI commands or Python testing: </p> <pre><code>$ wsl\n</code></pre> <p>The only exception to this is that <code>git</code> and <code>docker</code> commands should be run through a regular Windows commandline (not wsl). </p>"},{"location":"contributor/writing_building_documentation.html","title":"Writing and building documentation","text":"<p>The documentation is built using MkDocs with the Material for MkDocs theme.</p> <p>The documentation is written in Markdown format which is simple to learn and easy to read.</p>"},{"location":"contributor/writing_building_documentation.html#how-to-edit-the-documentation","title":"How to edit the documentation","text":"<p>All documentation is in the <code>docs</code> folder. The navigation structure is defined in <code>mkdocs.yml</code> at the project root.</p> <p>Edit or add files in this directory to edit the documentation. When adding new files, remember to add them to the <code>nav</code> section in <code>mkdocs.yml</code>.</p>"},{"location":"contributor/writing_building_documentation.html#how-to-build-the-documentation-locally","title":"How to build the documentation locally","text":"<p>From the project root, run:</p> <pre><code>make docs\n</code></pre> <p>Or directly with MkDocs:</p> <pre><code>uv run mkdocs build\n</code></pre> <p>The built documentation will be in the <code>site</code> directory. Open <code>site/index.html</code> to view it.</p>"},{"location":"contributor/writing_building_documentation.html#live-preview","title":"Live preview","text":"<p>For a live preview that auto-reloads when you make changes:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>Then open http://127.0.0.1:8000 in your browser.</p>"},{"location":"contributor/writing_building_documentation.html#api-documentation","title":"API Documentation","text":"<p>API documentation is auto-generated from Python docstrings using mkdocstrings. The API reference is in <code>docs/api/index.md</code>.</p>"},{"location":"external_models/index.html","title":"Documentation for Model Developers","text":"<p>The Chap platform brings several advantages to you as a model developer:</p> <ul> <li>It provides a broad range of supporting functionality for modelling, allowing you to focus on what is unique in your project, while relying on Chap for data parsing, model tuning, rigorous model assessment and more.</li> <li>It allows you to evaluate your model side-by-side with a range of other models in the field</li> <li>It comes with native connection to DHIS2 and the DHIS2 Modelling App, and includes mechanisms to disseminate your model to a large number of health ministries that use DHIS2 as their national health information system.</li> </ul> <p>We provide more detailed documentation depending on who you are:</p> <ul> <li>Experienced climate health modellers who want to develop or adapt existing models to be compatible with Chap.</li> <li>Someone new to modelling that wants to learn the principles of spatiotemporal modelling and how to more easily implement such models through Chap.</li> </ul>"},{"location":"external_models/chap-dhis2-connection.html","title":"Chap's native connection to DHIS2 and the DHIS2 Modelling App","text":"<p>Chap comes with native connection to DHIS2 and the DHIS2 Modelling App as described on the main DHIS2 climate web page</p>"},{"location":"external_models/chap_evaluate_examples.html","title":"Examples of chap evaluate commands","text":"<p>The following are examples of running various chap-integrated models on various datasets:</p> <ul> <li>minimalist_example_r:  <code>chap evaluate --model-name https://github.com/dhis2-chap/minimalist_example_r --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2</code></li> <li>minimalist_multiregion_r:  <code>chap evaluate --model-name https://github.com/dhis2-chap/minimalist_multiregion_r --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2</code></li> <li>minimalist_example_lag_r:  <code>chap evaluate --model-name https://github.com/dhis2-chap/minimalist_example_lag_r --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2</code></li> <li>Madagascar_ARIMA:  <code>chap evaluate --model-name https://github.com/dhis2-chap/Madagascar_ARIMA --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2</code></li> <li>Epidemiar: <code>chap evaluate --model-name https://github.com/dhis2-chap/epidemiar_example_model --dataset-csv ../epidemiar_example_data/input/laos_test_data.csv --report-filename report.pdf --debug --n-splits=2</code></li> <li>chap_auto_ewars_weekly: <code>chap evaluate --model-name https://github.com/dhis2-chap/chap_auto_ewars_weekly --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=1</code></li> <li>chap_auto_ewars: <code>chap evaluate --model-name https://github.com/dhis2-chap/chap_auto_ewars --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=1</code></li> </ul> <p>Note that the Epidemiar command uses a local file path for the supplied dataset as it requires weekly data, which is not currently available in CHAP's internal datasets. The command above works when cloning the epidemiar_example_model locally and if the command is run from the folder chap-core, then it assumes that the cloned repository is in the same folder as chap-core, and we use the relative file path. You can also simply dowload the csv file laos_test_data.csv from the github folder and reference the path to the local file.</p> <p>chap_auto_ewars only accepts mothly data while chap_auto_ewars_weekly can use both weekly and monthly data, should be combined together soon. Additionaly there is a version of chap_auto_ewars which uses spatial smoothing and a geojson file which can be ran as * <code>chap evaluate --model-name https://github.com/Halvardgithub/chap_auto_ewars --dataset-csv ../chap_auto_ewars/example_data_Viet/historic_data.csv --polygons-json ../chap_auto_ewars/example_data_Viet/vietnam.json --report-filename report.pdf --debug --n-splits=1 --polygons-id-field VARNAME_1</code> The above uses local files and their relative paths, the files are available at the github url.</p>"},{"location":"external_models/chap_evaluate_examples.html#for-windows-users","title":"For Windows users","text":"<p>Windows users might have issues with the commands above. The solution is to clone the repositories for the external models and add the optional command <code>--run-directory-type use_existing</code>. An example is shown below. * minimalist_example_r: <code>chap evaluate --model-name /mnt/c/Users/NAME/Documents/GitHub/minimalist_example_r/ --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2 --run-directory-type use_existing</code></p> <p>Note that you need to use your own local file path, and if you are using WSL and ubuntu this might be with <code>mnt</code> from linux, even on a Windows system. </p>"},{"location":"external_models/chap_evaluate_examples.html#warnings","title":"Warnings","text":"<p>When running the command with a local file path for the model folder you can in theory run the command from any folder, not just from chap-core. However, running <code>chap evaluate</code> with <code>--run-directory-type use_existing</code> from the same folder as you are using as the <code>--model-name</code> will cause an inifnite copying loop. Sometimes, if a command fails, it might be neccessary to exit and open the folder again, for example run <code>cd ../chap-core</code> to go one folder up and then back to chap-core. Additionaly, having an active VPN can aslo confuse CHAP and cause the commands to fail.</p>"},{"location":"external_models/chapkit.html","title":"Running models with chapkit","text":"<p>Chapkit is a new experimental way of integrating and running models through chap.</p> <p>In contrast to the current chap implementation, where models are run directly through docker and pyenv wrappers inside chap, chapkit models are separate REST API services that chap interacts with through HTTP requests.</p> <p>This has several advantages:</p> <ul> <li>There are no docker-in-docker problems where chap (which is inside docker) has to run docker commands to start model containers.</li> <li>We have a strict and clear API for how chap interacts with models, which makes it easier to develop and maintain.</li> <li>Models can easily be distributed through docker images or other means, as long as they implement the chapkit API. It is up to the model to define how it is deployed and run, but chapkit makes it easy to spin up a rest api and make a docker image for a model.</li> </ul> <p>This is still experimental and under development, but we have a working prototype with a few models already.</p> <p>This document describes very briefly how to make a model compatible with chapkit, and how to use chapkit models in chap.</p>"},{"location":"external_models/chapkit.html#how-to-make-a-model-compatible-with-chapkit","title":"How to make a model compatible with chapkit","text":"<p>This guide is not written yet, for now we refer to the chapkit documentation: https://dhis2-chap.github.io/chapkit/.</p>"},{"location":"external_models/chapkit.html#data-format-sent-to-chapkit-models","title":"Data format sent to chapkit models","text":"<p>CHAP sends data to chapkit models via the REST API in a standardized format. The data is sent as JSON with a <code>columns</code> array and a <code>data</code> array (column-oriented format).</p>"},{"location":"external_models/chapkit.html#required-columns","title":"Required columns","text":"<p>The following columns are always present in the training and prediction data:</p> Column Description <code>time_period</code> Time period identifier (see format below) <code>location</code> Location identifier <code>disease_cases</code> Number of disease cases (training data only) <code>rainfall</code> Rainfall measurement <code>mean_temperature</code> Mean temperature <code>population</code> Population count"},{"location":"external_models/chapkit.html#time-period-format","title":"Time period format","text":"<p>The <code>time_period</code> column uses ISO-like string formats:</p> <p>Weekly data: <pre><code>2020-W01\n2020-W02\n2020-W52\n2021-W01\n</code></pre></p> <p>The format is <code>YYYY-Wnn</code> where: - <code>YYYY</code> is the ISO week year - <code>W</code> indicates weekly data - <code>nn</code> is the zero-padded week number (01-53)</p> <p>Monthly data: <pre><code>2020-01\n2020-02\n2020-12\n2021-01\n</code></pre></p> <p>The format is <code>YYYY-MM</code> where: - <code>YYYY</code> is the year - <code>MM</code> is the zero-padded month number (01-12)</p>"},{"location":"external_models/chapkit.html#example-training-data-weekly","title":"Example training data (weekly)","text":"<pre><code>{\n  \"columns\": [\"time_period\", \"location\", \"disease_cases\", \"rainfall\", \"mean_temperature\", \"population\"],\n  \"data\": [\n    [\"2020-W01\", \"district_a\", 150, 45.2, 28.5, 50000],\n    [\"2020-W02\", \"district_a\", 142, 52.1, 27.8, 50000],\n    [\"2020-W01\", \"district_b\", 89, 38.7, 29.1, 35000],\n    [\"2020-W02\", \"district_b\", 95, 41.3, 28.9, 35000]\n  ]\n}\n</code></pre>"},{"location":"external_models/chapkit.html#example-training-data-monthly","title":"Example training data (monthly)","text":"<pre><code>{\n  \"columns\": [\"time_period\", \"location\", \"disease_cases\", \"rainfall\", \"mean_temperature\", \"population\"],\n  \"data\": [\n    [\"2020-01\", \"district_a\", 580, 180.5, 28.2, 50000],\n    [\"2020-02\", \"district_a\", 620, 165.3, 27.9, 50000],\n    [\"2020-01\", \"district_b\", 340, 155.8, 29.0, 35000],\n    [\"2020-02\", \"district_b\", 365, 148.2, 28.7, 35000]\n  ]\n}\n</code></pre>"},{"location":"external_models/chapkit.html#run-info","title":"Run info","text":"<p>Along with the data, CHAP sends a <code>run_info</code> object containing runtime parameters:</p> <pre><code>{\n  \"prediction_length\": 3,\n  \"additional_continuous_covariates\": [\"humidity\"]\n}\n</code></pre> Field Description <code>prediction_length</code> Number of future periods to predict <code>additional_continuous_covariates</code> List of additional covariate columns in the data"},{"location":"external_models/chapkit.html#how-to-run-a-chapkit-model-from-the-command-line-with-chap-evaluate","title":"How to run a chapkit model from the command line with chap evaluate","text":"<p>To test that the model is working with chap, you can use the <code>chap evaluate</code> command. Instead of a github url or model name, you simply specify the REST API url to the model and add --is-chapkit-model to the command to tell chap that the model is a chapkit model.</p> <p>Example:</p> <p>First save this model configuration to a file called <code>testconfig.yaml</code>:</p> <pre><code>user_option_values:\n  max_epochs: 2\n</code></pre> <p>Then start the chtorch command on port 5001:</p> <pre><code>docker run -p 5001:8000 ghcr.io/dhis2-chap/chtorch:chapkit2-8f17ee3\n</code></pre> <p>Then we can run the following command to evaluate the model. Note the http://localhost:5001 url, which tells chap to look for the model at that url.</p> <pre><code>chap evaluate --model-name http://localhost:5001 --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2 --model-configuration-yaml testconfig.yaml --prediction-length 3 --is-chapkit-model\n</code></pre>"},{"location":"external_models/chapkit.html#how-to-use-chapkit-models-in-chap-with-the-modeling-app","title":"How to use chapkit models in chap with the modeling app","text":"<p>NOTE: This is experimental, and the way this is done might change in the future.</p> <p>1) Add your model to a file compose-models.yml, pick a port for your model that is not used by other models 2) Add your model to a config file inside config/configured_models (e.g. config/configured_models/local_config.yaml), with the url pointing to your model, using the container name you specified in compose-models, e.g. http://chtorch:5001 (localhost will not work for communication between the chap worker container and your model container). Here is an example:   <code>yaml - url: http://chtorch:8000   uses_chapkit: true   versions:     v1: \"/v1\"   configurations:     debug:       user_option_values:           max_epochs: 2</code> 3) Start both chap and your model by running <code>docker compose -f compose.yml -f compose-models.yml up --build --force-recreate</code></p> <p>Now, the model should show up in the modeling app.</p>"},{"location":"external_models/describe_model.html","title":"Describing your model in our yaml-based format","text":"<p>To make your model chap-compatible, you need your train and predict endpoints (as discussed here) need to be formally defined in a YAML format that follows the popular MLflow standard. Your codebase need to contain a file named <code>MLproject</code> that defines the following: - An entry point in the MLproject file called <code>train</code> with parameters <code>train_data</code> and <code>model</code> - An entry point in the MLproject file called <code>predict</code> with parameters <code>historic_data</code>, <code>future_data</code>, <code>model</code> and <code>out_file</code></p> <p>These should contain commands that can be run to train a model and predict the future using that model. The model parameter should be used to save a model in the train step that can be read and used in the predict step. CHAP will provide all the data (the other parameters) when running a model.</p> <p>Here is an example of a valid MLproject file (taken from our minimalist_example).</p> <p>The MLproject file can specify a docker image or Python virtual environment that will be used when running the commands.  An example of this is the MLproject file contained within our minimalist_example_r.</p>"},{"location":"external_models/experienced_modeller.html","title":"Documentation for experienced modellers","text":"<p>The following are required to develop or integrate an existing model with Chap:</p> <ul> <li>Make chap-compatible train and predict endpoints (functions) for you model that accepts the standard chap data formats</li> <li>Describe your model in a simple yaml-based format</li> <li>Check that your model runs through Chap</li> </ul> <p>To get started, we recommend to follow our simple tutorial:</p> <ol> <li>Clone or download our minimalist example and make sure you can run the minimalist code used in the tutorial (as an isolated run and through Chap)</li> <li>Replace the code in the train and predict functions with the code of your own model, doing any necessary data format conversion to make your model compatible with the Chap setup</li> </ol> <p>If you are more comfortable with R than Python, you can alterantive clone/download our R-based minimalist example</p> <p>If you are unsure on some of the involved IT technologies (like version control, containerisation etc), please consult our Introduction to Development Tools</p>"},{"location":"external_models/learn_modelling.html","title":"Learning statistical and machine learning modelling - with climate-sensitive disease forecasting as focus and case","text":"<p>This material will gradually introduce you to important concepts from statistical modelling and machine learning,  focusing on what you will need to understand in order to do forecasting of climate-sensitive disease. It thus selects a set of topics needed for disease forecasting, while mostly introducing these concepts in generality.</p> <p>The material is organised around hands-on tutorials, where you will learn how to practically develop models while learning the theoretical underpinnings.</p>"},{"location":"external_models/learn_modelling.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: you must know some programming, in either Python or R, to be able to follow our exercises and tutorials. </li> <li>Data science: You should know basic data science and statistics or machine learning. However, very little is presumed as it can be learnt underway, but one should know the most basic concepts and terminology.</li> <li>GitHub: Central to our approach is that you follow various practical tutorials along the way. These tutorials are available at GitHub, so you need to know at least the absolute basics of how to get code from GitHub to your local machine - if not please ask us for tips on Github tutorials.</li> </ul>"},{"location":"external_models/learn_modelling.html#background","title":"Background","text":"<p>Climate change is rapidly reshaping the patterns and spread of disease, posing urgent challenges for health systems. To effectively respond, the health systems must become more adaptive and data-driven. Early warning and response systems (EWS) that leverage disease incidence forecasting offer a promising way to prioritize interventions where and when they are most needed.</p> <p>At the core of early warning is forecasting of disease incidence forward in time. This is based on learning a statistical/machine learning model of how disease progresses ahead in time based on various available data. </p> <p>If you have limited prior experience with statistics or machine learning, please read our brief intro in the expandable box below:</p> A gentle introduction to statistical and time series modelling"},{"location":"external_models/learn_modelling.html#1-statistical-or-machine-learning-model","title":"1. Statistical or Machine Learning Model","text":"<p>A model is a rule or formula we create to describe how some outcome depends on information we already have.</p> <ul> <li>The outcome we want to understand or predict is often called the target.</li> <li>The information we use to make that prediction is called predictors (or features).</li> </ul> <p>A model tries to capture patterns in data in a simplified way. You can think of a model as a machine:</p> <p>input (predictors) \u2192 model learns a pattern \u2192 output (prediction)</p> <p>The goal is either to explain something (\u201cWhat affects sales?\u201d) or to predict something (\u201cWhat will sales be tomorrow?\u201d).</p>"},{"location":"external_models/learn_modelling.html#2-predictors-features","title":"2. Predictors (Features)","text":"<p>A predictor is any variable that provides information helpful for predicting the target.</p> <p>Examples: - Temperature when predicting ice cream sales - Age when predicting income - Yesterday\u2019s stock price when predicting today\u2019s  </p> <p>Predictors are the model\u2019s inputs. We usually write them as numbers: - A single predictor as x - Several predictors as x\u2081, x\u2082, x\u2083, \u2026  </p> <p>The model learns how each predictor is related to the target.</p>"},{"location":"external_models/learn_modelling.html#3-linear-regression","title":"3. Linear Regression","text":"<p>Linear regression is one of the simplest and most widely used models. It assumes that the target is approximately a straight-line combination of its predictors.</p> <p>With one predictor x, the model is:</p> <p>prediction = a + b\u00b7x</p> <ul> <li>a is the model\u2019s baseline (what we predict when x = 0)  </li> <li>b tells us how much the prediction changes when x increases by 1 unit  </li> </ul> <p>With multiple predictors x\u2081, x\u2082, \u2026, we extend the same idea:</p> <p>prediction = a + b\u2081\u00b7x\u2081 + b\u2082\u00b7x\u2082 + \u2026</p> <p>You don\u2019t need to imagine shapes in many dimensions\u2014just think of it as a recipe where each predictor gets a weight (b) that shows how important it is.</p> <p>The model \u201clearns\u201d values of a, b\u2081, b\u2082, \u2026 by choosing them so that predictions are as close as possible to the observed data.</p>"},{"location":"external_models/learn_modelling.html#4-time-series","title":"4. Time Series","text":"<p>A time series is a sequence of data points collected over time, in order:</p> <p>value at time 1, value at time 2, value at time 3, \u2026</p> <p>Examples: - Daily temperatures - Hourly website traffic - Monthly number of customers  </p> <p>What makes time series special is that:</p> <ul> <li>The order matters </li> <li>Past values can influence future values </li> <li>Data may show patterns such as trends (general increase/decrease over time) or seasonality (repeating patterns, like higher electricity use every winter)</li> </ul>"},{"location":"external_models/learn_modelling.html#5-time-series-forecasting","title":"5. Time Series Forecasting","text":"<p>Forecasting means using past observations to predict future ones.</p> <p>Unlike models that treat each data point separately, forecasting models learn ideas like:</p> <ul> <li>how the series tends to move (trend)  </li> <li>whether it repeats patterns (seasonality)  </li> <li>how strongly the recent past influences the next value  </li> </ul> <p>A simple forecasting idea is to predict the next value using a weighted average of recent past values. More advanced methods learn more complex patterns automatically.</p>"},{"location":"external_models/learn_modelling.html#6-evaluation-of-predictions","title":"6. Evaluation of Predictions","text":"<p>Once a model makes predictions, we need to measure how good they are. This means comparing the model\u2019s predictions to the actual values.</p> <p>Let: - actual value = y - predicted value = \u0177 (read as \u201cy-hat\u201d)</p> <p>The error is:</p> <p>error = actual \u2212 predicted = y \u2212 \u0177</p> <p>Common ways to summarize how large the errors are:</p> <ul> <li>MAE (Mean Absolute Error):   average of |y \u2212 \u0177| (the average size of the mistakes)</li> <li>MSE (Mean Squared Error):   average of (y \u2212 \u0177)\u00b2 (large mistakes count extra)</li> <li>RMSE (Root Mean Squared Error):   the square root of MSE (in the same units as the data)</li> <li>MAPE (Mean Absolute Percentage Error):   how large the errors are relative to the actual values, in %</li> </ul> <p>These measures help us compare models and choose the one that predicts best.</p> <p>For a bit more in-depth introduction, please also consider the following general papers:</p> <ul> <li>Machine learning: A primer</li> <li>Simple linear regression</li> <li>Multiple linear regression</li> </ul>"},{"location":"external_models/learn_modelling.html#motivation","title":"Motivation","text":"<p>Our tutorial aims to introduce aspects of statistical modelling and machine learning that are useful specifically for developing, evaluating and later operationalising forecasting models. Our pedagogical approach is to begin by introducing a very simple model in a simple setting, and then expanding both the model and the setting in a stepwise fashion. We emphasize interoperability and rigorous evaluation of models right from the start, as a way of guiding the development of more sophisticated models. In doing this, we follow a philosophy resembling what is known as agile development in computer science. To facilitate interoperability and evaluation of models, we rely on the Chap platform, which enforces standards of interoperability already from the first, simple model. This interoperability allows models to be run on a broad data collection and be rigorously evaluated with rich visualisations of data and predictions already from the early phase.</p>"},{"location":"external_models/learn_modelling.html#making-your-first-model-and-getting-it-into-chap","title":"Making your first model and getting it into chap","text":"<p>Disease forecasting is a type of problem within what is known as spatiotemporal modelling in the field of statistics/ML. What this means is that the data have both a temporal and spatial reference (i.e. data of disease incidence is available at multiple subsequent time points, for different regions in a country), where observations that are close in time or space are usually considered more likely to be similar. In our case, we also have data both on disease and on various climate variables that may influence disease incidence.</p> <p>Before going into the many challenges of spatiotemporal modelling, we recommend that you get the technical setup in place to allow efficient development and learning for the remainder of this tutorial. Although this can be a bit of a technical nuisance just now, it allows you to run your model on a variety of data inputs with rich evaluation now already, and it allows you to progress efficiently with very few technical wrinkles as the statistical and ML aspects become more advanced. To do this, please follow our minimalist example tutorial, which introduces an extremely oversimplified statistical model (linear regression of immediate climate effects on disease only), but shows you how to get this running in Chap.  This minimalist tutorial is available both as Python and as R code:</p> <ul> <li>Minimalist Example (Python) </li> <li>Minimalist Example (R) </li> </ul>"},{"location":"external_models/learn_modelling.html#evaluating-a-model","title":"Evaluating a model","text":"<p>The purpose of spatiotemporal modelling is to learn generalisable patterns that can be used to reason about unseen regions or about the future. Since our use case is an early warning system, our focus is on the latter, i.e. forecasting disease incidence ahead in time based on historic data for a given region. Therefore, we will focus on evaluating a model through its forecasting skill into the future.</p> <p>A straightforward way to assess a forecasting model is to create and record forecasts for future disease development, wait to see how disease truly develops, and then afterwards compare the observed numbers to what was forecasted. This approach has two main limitations, though:  it requires to wait through the forecast period to see what observations turn out to be it only shows the prediction skill of the forecast model at a single snapshot in time - leaving a large uncertainty on how a system may be expected to behave if used to forecast new future periods. </p> <p>A popular and powerful alternative is thus to perform what is called backtesting or hindcasting: one pretends to be at a past point in time, providing a model exclusively with data that was available before this pretended point in time, making forecasts beyond that time point (for which no information was available to the model), and then assessing how close this forecast is to what happened after the pretended time point. When performed correctly, this resolves the both mentioned issues with assessing forecasts truly made into the future:  Since observations after the pretended time point is already available in historic records, assessment can be performed instantaneously, and one can choose several different pretended time points, reducing uncertainty of the estimated prediction skill  and also allowing to see variability in prediction skill across time. </p> <p>To be truly representative of true future use, it is crucial that the pretended time point for forecasting realistically reflects a situation where the future is not known. There are a myriad pitfalls in assessment setup that can lead to the assessment not respecting the principle of future information beyond the pretended time point not being accessible to models. This is discussed in more detail in the document \"Ensuring unbiased and operationally relevant assessments of climate-informed disease forecasting\".</p> <p>Prediction skill can be measured in different ways. One simple way to measure this is to look at how far off the predictions are, on average, from the true values (known as mean absolute error, MAE). Other common measures are discussed later in this tutorial, after we have introduced further aspects of modelling.  To make the most of the data we have, we often use a method called cross-validation. This means we repeatedly split the data into \u201cpast\u201d and \u201cfuture\u201d parts at different time points. We then make forecasts for each split and check how accurate those forecasts are. This helps us see how well the model performs across different periods of time. To learn more, Wikipedia has a broad introduction to this topic, including specifics for time series models. </p> <p>Since we are here running our models through Chap, we can lean on an already implemented solution to avoid pitfalls and ensure a data-efficient and honest evaluation of the models we develop. Chap also includes several metrics, including MAE, and offers several convenient visualisations to provide insights on the prediction skill.  To get a feeling for this, please follow the code-along tutorials on assessment with Chap. We recommend to start with our simple code-along tutorial for how to split data and compute MAE on the pretended future. </p> <p>After getting a feeling for assessment, please continue with our code tutorial on how to use the built-in chap evaluation functionality to perform more sophisticated evaluation of any model (rather than implementing your own simple evaluation from scratch, as in the previous code-along tutorial).</p> <p>With this evaluation setup in place, you should be ready to start looking at more sophisticated modelling approaches. For each new version of your model, evaluation helps you check that the new version is actually an improvement (and if so, in which sense, for instance short-term vs long-term, accuracy vs calibration, large vs small data set).</p>"},{"location":"external_models/learn_modelling.html#expanding-your-model-to-make-it-more-meaningful","title":"Expanding your model to make it more meaningful","text":""},{"location":"external_models/learn_modelling.html#multiple-regions","title":"Multiple regions","text":"<p>While it may in some settings be useful to forecast disease at a national level, it is often more operationally relevant to create forecasts for smaller regions within the country, for instance at district level. Therefore, a disease forecasting approach needs to relate to multiple districts in the code, to create forecasts per district.  If a single model is trained across district data (ignoring the fact that there are different districts) and used to directly forecast disease without taking into account differences between districts in any way, it would forecast similar levels of disease across districts regardless of disease prevalence in each particular district.  To see this more concretely please follow this tutorial to see the errors that the minimalist_example model (which ignores districts) makes for the two districts D1 and D2 with respectively high and low prevalence.</p> <p>The simplest approach to creating meaningful region-level forecast is to simply train and forecast based on a separate model per region. Please follow the tutorial below (in Python or R version) to see an easy way of doing this in code: </p> <ul> <li>Minimalist Multiregion (Python)</li> <li>Minimalist Multiregion (R)</li> </ul> <p>However, such independent consideration of each region also has several weaknesses. A main weakness is connected to the amount of data available to learn a good model. When learning a separate model per region, there may be a scarcity of data to fit a model of the desired complexity. This relates to a general principle in statistics concerning the amount of data available to fit a model versus the number of parameters in a model (see e.g. this article). More available data allows for a larger number of parameters and a more complex model. Compared to the case with separate models per region, just combining data across all regions into a single model where the parameters are completely independent between districts does not change the ratio of available data versus parameters to be estimated. However, if the parameters are dependent (for example due a spatial structure, i.e. similarities between regions), the effective number of parameters will be lower than the actual number. There is a trade-off between having completely independent parameters on one hand and, and forcing parameters to be equal across regions on the other hand. This is often conveyed as \u201cborrowing strength\u201d between regions. It is also related to the concept of the bias-variance tradeoff in statistics and machine learning (see e.g. this Wikipedia article), where dependency between parameter values across regions introduces a small bias in each region towards the mean across regions, while reducing variance of the parameter estimates due to more efficient use of data. </p> <p>As the disease context (including numbers of disease cases) can vary greatly between regions, the same type of model is not necessarily suited for all regions. However, taking this into account can be complex, especially if one wants to combine flexibility of models with the concept of borrowing strength mentioned above. It is thus often more practical to use a single model across regions, but ensure that this model is flexible enough to handle such heterogeneity across regions. </p>"},{"location":"external_models/learn_modelling.html#lags-and-autoregressive-effect","title":"Lags and autoregressive effect","text":"<p>The incidence of disease today is usually affected by the incidence of disease in the recent past. This is for instance almost always the case with infectious disease, whether or not transmission is directly  human-to-human or via a vector like mosquitoes (e.g. Malaria and Dengue). Thus, it is usually advisable to include past cases of disease as a predictor in a model. This is typically referred to as including autoregressive effects in models.</p> <p>Additionally, climate variables such as rainfall and temperature usually don\u2019t have an instantaneous effect. Typically, there is no way that (for example) rainfall today would influence cases today or in the nearest days. Instead, heavy rainfall today could alter standing water, affecting mosquito development and behavior, with effects on reported Malaria cases being several weeks ahead. This means that models should typically make use of past climatic data to predict disease incidence ahead. The period between the time point that a given data value reflects and the time point of its effect is referred to as a lag. The effect of a predictor on disease may vary and even be opposite when considered at different lags. A model should usually include predictors at several different time lags, where the model aims to learn which time lags are important and what the effects are for a given predictor at each such lag.</p> <p>Available data can be on different time resolutions. In some contexts, reported disease cases are available with specific time stamps, but more often what is collected is aggregated and made available at weekly or monthly resolution. The available data resolution influences how precisely variation in predictor effects across lag times can be represented. </p> <p>Basically, the influence of each predictor at each time lag will be represented by a separate parameter in a model. As discussed in the previous section on multiple regions, this can lead to too many parameters to be effectively estimated. It is thus common to employ some form of smoothing of the effects of a predictor across lags, with a reasoning similar to that of borrowing strength across regions (here instead borrowing strength across lag times).</p> <p>At a practical level, model training is often assuming that all predictors to be used for a given prediction (including lagged variables) are available at the same row of the input data table. It is thus common to modify the data tables to make lagged predictors line up. </p> <p>To see a simple way of doing this in code, follow this tutorial in Python or R:</p> <ul> <li>Minimalist Example Lag (Python)</li> <li>Minimalist Example Lag (R)</li> </ul>"},{"location":"external_models/learn_modelling.html#expanding-your-model-with-uncertainty","title":"Expanding your model with uncertainty","text":"<p>Future disease development can never be predicted with absolute certainty - even for short time scales there will always be some uncertainty of how the disease develops. For a decision maker, receiving a point estimate (a single best guess) without any indication of uncertainty is usually not very useful. Consider a best guess of 100 cases in a region. The way to use this information would likely be very different if the model predicted a range from 95 to 105 vs a range from 0 to 10 000. Although the range provided by the model is itself uncertain, it still provides a useful indication of what can be expected, and it will be possible to evaluate its ability to report its own uncertainty (as will be explained in the next section). </p> <p>When doing statistical modelling, uncertainty arises from several distinct sources, many of which are particularly important in time series settings. First, observed data are often noisy or imperfect measurements of an underlying process. Measurement error, reporting delays, missing values, or later data revisions mean that the observed series does not exactly reflect the true system being modelled.</p> <p>Even with perfect observations, the underlying process itself is usually stochastic, meaning that even if one had precise measurements of the current status and had learnt a perfectly correct model that involves these measured data, a variety of external factors would influence disease development in unpredictable ways. Additional uncertainty comes from estimating model parameters (relations between climate and disease) using often limited historical data. This uncertainty propagates into predictions. There is also uncertainty associated with model choice. Any model is a simplification of reality, and incorrect assumptions about lag structure, stationarity, seasonality, or linearity can lead to biased inference and misleading forecasts. This structural uncertainty is difficult to quantify but often dominates in practice.</p> <p>Bayesian modelling provides a way to combine uncertainties from various sources in a coherent, principled way, making sure that the overall uncertainty is well represented. One aspect of this is that instead of estimating one particular value for each of its model parameters (e.g. for the relation between a given climate factor and the disease), it estimates a probability distribution for this value (referred to as the posterior distribution). Uncertainty intervals can then be based on the posterior distribution, for example using the interval between the 5% and 95% quantiles would give an interval with 90% probability of containing the true value of the parameter. Similarly, prediction intervals can be generated for the disease forecasts based on the uncertainty represented in the model itself (in its parameter distributions).</p> <p>Many other modelling approaches (e.g. classical ARIMA and most machine learning models) are mainly focused on the trend of disease development - of predicting a point estimate (single value) for the expected (or most likely) number of disease cases ahead. Uncertainty is then often added on top of this prediction of expected value. A simple choice is to view the model predictions as representing the expected value of some probability distribution that captures the uncertainty of the forecast. For instance, one could view model forecasts as representing the expected value of a normal distribution with a fixed standard deviation that is estimated to capture the uncertainty as well as possible (under such an assumption of normality). Another more sophisticated alternative is to model this standard deviation itself, so that for every forecast, both the expected value and the uncertainty of the forecast is influenced by the available data.</p> <p>Reporting forecasted point estimates is straightforward - it is just to report a single number per region and time period. Reporting a prediction that includes uncertainty is less trivial. If the uncertainty is captured by a given parametric distribution (like a plain normal distribution), one could simply report the parameters of this distribution (the mean and standard deviation in the case of the normal distribution). When uncertainty may follow different distributions, one needs a more flexible approach. One possibility is to report quantiles of this distribution - e.g. the 10% and 90% quantiles, which allows to see a 80% prediction interval. One could also report many of these. An even more generic approach to this is to report samples from the distribution, which allows the full distribution to be approximately reconstructed from the samples. It also allows any quantile to be approximated by simply sorting the samples and finding the value that a given proportion of samples are below. Due to its flexibility, this representation is often the preferred and chosen approach to represent predicted distributions, and is also what is used in the chap models when models report their forecasts back to the platform. </p>"},{"location":"external_models/learn_modelling.html#evaluating-predicted-uncertainty","title":"Evaluating predicted uncertainty","text":"<ul> <li>The concept of uncertainty calibration (and why it is operationally important)</li> </ul>"},{"location":"external_models/learn_modelling.html#honest-and-sophisticated-evaluation-in-more-detail","title":"Honest and sophisticated evaluation in more detail","text":"<ul> <li>(see also separate manuscript about this..)</li> <li>Time series cross-validation.. (growing/rolling window)</li> </ul>"},{"location":"external_models/learn_modelling.html#relating-to-data-quality-issues-including-missing-data","title":"Relating to data quality issues, including missing data","text":""},{"location":"external_models/learn_modelling.html#more-realistic-sophisticated-models","title":"More realistic (Sophisticated) models","text":"<ul> <li>BayesianHierarchicalModelling, including INLA, mentioning STAN</li> <li>Auto-regressive ML</li> <li>ARIMA and variants</li> <li>Deep learning</li> </ul>"},{"location":"external_models/learn_modelling.html#systematic-evaluation-on-simulated-data-for-debugging-understanding-and-stress-testing-models","title":"Systematic evaluation on simulated data for debugging, understanding and stress-testing models","text":"<ul> <li>Simulating from the model or something presumed close to the model - see if the model behaves as expected and how much data is needed to reach desired behavior..</li> <li>Simulate specific scenarios to stress-test models</li> <li>Develop more realistic simulations</li> </ul>"},{"location":"external_models/learn_modelling.html#selecting-meaningful-features-covariates-and-data","title":"Selecting meaningful features (covariates) and data","text":""},{"location":"external_models/learn_modelling.html#further-resources","title":"Further resources","text":"<p>Some general resources - not properly screened for now:</p> <ul> <li>Introduction to Statistical Modeling, Hughes and Fisher, 2025</li> <li>An Introduction to Statistical Learning, James et. al., 2021</li> <li>Intro to Time Series Forecasting</li> <li>Time Series Forecasting: Building Intuition</li> <li>Regularization</li> <li>Importance of being uncertain</li> </ul>"},{"location":"external_models/newcomer.html","title":"Effectively learn modelling using Chap","text":"<p>CHAP is designed to allow model developers to focus on their core modelling idea, and then rely on Chap for rich support of data, evaluation, interoperability and more.  We here provide guides to learn modelling through example models in either Python and R.</p> <p>We have a separate guide for those who already know modelling, while we here provide guides based on example code to </p> <ul> <li>Learn both theoretical and practical aspects of modelling, leaning on Chap</li> </ul>"},{"location":"external_models/running_models_in_chap.html","title":"Running models through Chap","text":"<p>In order to run Chap, you should first follow our guide for how to install Chap.  </p>"},{"location":"external_models/running_models_in_chap.html#running-models-through-the-chap-command-line-interface","title":"Running models through the Chap command-line interface","text":"<p>Models that are compatible with CHAP can be used with the <code>chap evaluate</code> command. An external model can be provided to CHAP in two ways: - By specifying a path to a local code base: <pre><code>$ chap evaluate --model-name /path/to/your/model/directory --dataset-name ISIMIP_dengue_harmonized --dataset-country brazil --report-filename report.pdf --ignore-environment  --debug\n</code></pre> - By specifying a github URL to a git repo (the url needs to start with https://github.com/): <pre><code>$ chap evaluate --model-name https://github.com/dhis2-chap/minimalist_example --dataset-name ISIMIP_dengue_harmonized --dataset-country brazil --report-filename report.pdf --ignore-environment  --debug\n</code></pre></p> <p>Note the <code>--ignore-environment</code> in the above commands.  This means that we don't ask CHAP to use Docker or a Python environment when running the model.  This can be useful when developing and testing custom models before deploying them to a production environment. Instead the model will be run directly using the current environment you are in.  This usually works fine when developing a model, but requires you to have both chap-core and the dependencies of your model available. </p> <p>As an example, the following command runs the chap_auto_ewars model on public ISMIP data for Brazil (this does not use --ignore-environment and will set up a docker container based on the specifications in the MLproject file of the model): <pre><code>$ chap evaluate --model-name https://github.com/dhis2-chap/chap_auto_ewars --dataset-name ISIMIP_dengue_harmonized --dataset-country brazil\n</code></pre></p> <p>If the above command runs without any error messages, you have successfully evaluated the model through CHAP, and a file <code>report.pdf</code> should have been generated with predictions for various regions.</p> <p>A folder <code>runs/model_name/latest</code> should also have been generated that contains copy of your model directory along with data files used. This can be useful to inspect if something goes wrong.</p>"},{"location":"external_models/running_models_in_chap.html#experimental-passing-model-specific-options-to-the-model","title":"(Experimental:) Passing model-specific options to the model","text":"<p>We are currently working on experimental functionality for passing options and other parameters through Chap to the model.</p> <p>The first way we plan to support this is when evaluating a model using the same <code>chap evaluate</code> command as described above.</p> <p>This functionality is under development. Below is a minimal working example using the model <code>naive_python_model_with_mlproject_file_and_docker</code>. This model has a user_option <code>some_option</code>, which we can specify in a yaml file:</p> <pre><code>chap evaluate --model-name external_models/naive_python_model_with_mlproject_file_and_docker/ --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --n-splits 2 --model-configuration-yaml external_models/naive_python_model_with_mlproject_file_and_docker/example_model_configuration.yaml\n</code></pre>"},{"location":"external_models/side_by_side_comparison.html","title":"Evaluating your model side-by-side with a range of other models","text":"<p>Since Chap integrates a large number of models under a unified interface, it is easy to compare the predictions of your model against those of alternative models using a variety of input data, metrics and visualisations.</p> <p>The most powerful way of comparing models is to install the Chap modelling platform alongside a DHIS2 instance, allowing you to run your and other models through the GUI of the \"Modelling App\", which includes interactive side-by-side comparison of predictions by different model.</p> <p>As a simpler setup, you can also evaluate your own and other models one by one using the Chap command line interface (the chap evaluate command), and then compare the resulting predictions in your preferred way. A range of existing models in the field has already been integrated with Chap, and can be run through the Modelling App or by using the following chap evaluate commands.</p>"},{"location":"external_models/supporting_functionality.html","title":"Chap's broad range of supporting functionality for modelling","text":"<p>Chap already contains rich functionality for data handling, data input from DHIS2 and rigorous model evaluation.  This allows modellers to focus only on training and predicting based on a single provided dataset,  while relying on the Chap framework to collect data from various sources, parse data of different formats, and perform multiple train-and-predict iterations as part of a rigorous time-series cross-validation. </p> <p>There are ongoing developments for a range of further supporting features, allowing modellers to rely on the Chap framework for model tuning (autoML), ensemble model learning, model explainability and more (please see overview of planned features here)   </p>"},{"location":"external_models/surplus_after_refactoring.html","title":"Integrating External Models with DHIS2 through CHAP","text":"<p>Assuming you have CHAP running on a server with DHIS2 (see this guide), it is possible to make new external models available.</p> <p>Currently, CHAP has an internal registry of models that can be used. If you want to run a model that is not in the registry, this now has to be done by editing the local CHAP code at the server. However, we are working on making this more flexible. For now, please reach out if you want a new model to be added to the internal registry in a given installation.</p> <p>The following figure shows how the train and predict entry points are part of a data flow between DHIS2 and the external model:</p> <p></p>"},{"location":"external_models/surplus_after_refactoring.html#overview-of-supported-models","title":"Overview of supported models","text":""},{"location":"external_models/surplus_after_refactoring.html#autoregressive-weekly","title":"Autoregressive weekly","text":"<p>This model has been developed internally by the Chap team. Autoregressive weekly is a deep learning model based on DeepAR that uses rainfall and temperature as climate predictors and models disease counts with a negative binomial distribution where the parameters are estimated by a recurrent neural network. The current model available through Chap can be found here.</p>"},{"location":"external_models/surplus_after_refactoring.html#epidemiar","title":"Epidemiar","text":"<p>Epidemiar is a Generalizes additive model (GAM) used for climate health forecasts. It requires weekly epidemilogical data, like disease cases and population, and daily enviromental data. As most of the data in CHAP is monthly or weekly we pass weakly data to the model, and then naively expand weekly data to daily data, which the epidemiar library again aggregates back to weekly data. The model produces a sample for each location per time point with and upper and lower boundary for some unknown quantiles. For more information regarding the model look here. An example of running the model from the CHAP command line interface is <pre><code>chap evaluate --model-name https://github.com/dhis2-chap/epidemiar_example_model --dataset-csv LOCAL_FILE_PATH/laos_test_data.csv --report-filename report.pdf --debug --n-splits=3\n</code></pre> which requires that <code>laos_test_data</code> is saved locally, but we are working on making weekly datasets available internaly in CHAP.</p>"},{"location":"external_models/surplus_after_refactoring.html#ewars","title":"EWARS","text":"<p>EWARS is a Bayesian hierarchical model implemented with the INLA library. We use a negative binomial likelihood in the observation layer and combine several latent effect, both spatial and temporal, in the latent layer. The latent layer is log-transformed and scaled by the population, so it effectivaly models the proprotion of cases in each region. Specifically the latent layers combine a first order cyclic random walk to capture the seasonal effect, this is also included in the lagged exogenous variables rainfall and temperature, then a spatial smoothing with an ICAR and an iid effect to capture the spatial heterogeneity. The ICAR and iid can also be combined, scaled and reparameterized to the BYM2 model. Further information is available in the model repository. An example of running the model from the CHAP command line interface is <pre><code>chap evaluate --model-name https://github.com/dhis2-chap/chap_auto_ewars --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=3\n</code></pre></p>"},{"location":"external_models/surplus_after_refactoring.html#arima","title":"ARIMA","text":"<p>A general ARIMA model is a timeseries model with an autoregressive part, a moving average part and the option to difference the original timeseries, often to make it stationary. Additonally we have lagged rainfall and temperature, which actually makes this an ARIMAX model, where the X indicates exogenous variables. This model handles each region individually and it expects monthly data for all the covariates. The model utilizes the <code>arima</code> function which chooses the order of the differencing, autoregression and the moving average for us. Further information is available in the model repository. An example of running the model from the CHAP command line interface is <pre><code>chap evaluate --model-name https://github.com/dhis2-chap/Madagascar_ARIMA --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=3\n</code></pre></p>"},{"location":"external_models/surplus_after_refactoring.html#wrapping-gluonts-models","title":"Wrapping GluonTS models","text":"<p>GluonTS provides a set of models that can be used for probabilistic time-series forecasting. Here, we show how we can wrap these models into CHAP models, to enable using them on spatio-temporal data and to evalutate them against other models.</p> <p>We will use the <code>DeepAREstimator</code> model from GluonTS, which is a deep learning model based on an RNN architecture. For this simple example we use a model that does not take weather into account, but only the the auto-regressive time series data. Let's start by loading the data and the model.</p> <pre><code>from chap_core.file_io.example_data_set import datasets\nfrom chap_core.adaptors.gluonts import GluonTSEstimator\nfrom gluonts.torch import DeepAREstimator\nfrom gluonts.torch.distributions import NegativeBinomialOutput\n\n# Load the data\ndata = datasets['ISIMIP_dengue_harmonized'].load()['vietnam']\n\n# Define the DeepAR model\nn_locations = len(data.locations)\nprediction_length = 4\ndeep_ar = DeepAREstimator(\n    num_layers=2,\n    hidden_size=24,\n    dropout_rate=0.3,\n    num_feat_static_cat=1,\n    scaling=False,\n    embedding_dimension=[2],\n    cardinality=[n_locations],\n    prediction_length=prediction_length,\n    distr_output=NegativeBinomialOutput(),\n    freq='M')\n\n# Wrap the model in a CHAP model\nmodel = GluonTSEstimator(deep_ar, data)\n</code></pre> <p>The model now is a chap compatible model and we can run our evaluation pipeline on it.</p> <pre><code>from chap_core.assessment.prediction_evaluator import evaluate_model\n\nevaluate_model(model, data, prediction_length=4, n_test_sets=8, report_filename='gluonts_deepar_results.csv')\n</code></pre>"},{"location":"external_models/surplus_after_refactoring.html#running-an-external-model-in-python","title":"Running an external model in Python","text":"<p>CHAP contains an API for loading models through Python. The following shows an example of loading and evaluating three different models by specifying paths/github urls, and evaluating those models:</p> <pre><code>import pandas as pd\n\nfrom chap_core.assessment.prediction_evaluator import evaluate_model\nfrom chap_core.models.utils import get_model_from_directory_or_github_url\nfrom chap_core.file_io.file_paths import get_models_path\nfrom chap_core.file_io.example_data_set import datasets\nimport logging\n\nif __name__ == '__main__':\n    logging.basicConfig(level=logging.INFO)\n    models_path = get_models_path()\n    model_names = {\n        #'deepar': models_path / 'deepar',\n        'naive_model': models_path / 'naive_python_model_with_mlproject_file',\n        # 'ewars': 'https://github.com/sandvelab/chap_auto_ewars'\n    }\n\n    dataset = datasets['ISIMIP_dengue_harmonized'].load()\n    dataset = dataset['vietnam']\n    n_tests = 7\n    prediction_length = 6\n    all_results = {}\n    for name, model_name in model_names.items():\n        model = get_model_from_directory_or_github_url(model_name)\n        results = evaluate_model(model, dataset,\n                                 prediction_length=prediction_length,\n                                 n_test_sets=n_tests,\n                                 report_filename=f'{name}_{n_tests}_{prediction_length}_report.pdf')\n        all_results[name] = results\n\n    report_file = 'evaluation_report.csv'\n    df = pd.DataFrame([res[0] | {'model': name} for name, res in all_results.items()])\n    df.to_csv(report_file, mode='w', header=True)\n</code></pre>"},{"location":"external_models/surplus_after_refactoring.html#docker-compose-chap-core","title":"Docker Compose (CHAP Core)","text":"<p>Starting CHAP Core using Docker Compose is specifically for those who want to use the CHAP Core REST-API, either together with other services or with the Modeling App installed on a DHIS2 server. See documentation for Modeling App for instructions on how to install the Modeling App.</p> <p>Requirements</p> <ul> <li>Access to credentials for Google Earth Engine. (Google Service Account Email and Private Key)</li> </ul>"},{"location":"external_models/surplus_after_refactoring.html#1-install-docker-if-not-installed","title":"1. Install Docker (if not installed)","text":"<p>Docker is a platform for developing, shipping, and running applications inside containers.</p> <p>To download and install Docker, visit the official Docker website: https://docs.docker.com/get-started/get-docker</p>"},{"location":"external_models/surplus_after_refactoring.html#2-clone-chap-core-github-repository","title":"2. Clone CHAP Core GitHub-Repository","text":"<p>You need to clone the CHAP Core repository from GitHub. Open your terminal and run the following command:</p> <pre><code>git clone https://github.com/dhis2-chap/chap-core.git\n</code></pre>"},{"location":"external_models/surplus_after_refactoring.html#3-add-credentials-for-google-earth-engine","title":"3. Add Credentials for Google Earth Engine","text":"<ol> <li>Open your terminal and navigate to the \"chap-core\" repository you cloned:</li> </ol> <pre><code>cd chap-core\n</code></pre> <ol> <li>Open the \"chap-core\" repository in your code editor. For example, if you are using Visual Studio Code, you can use the following command in the terminal:</li> </ol> <pre><code>code .\n</code></pre> <ol> <li> <p>In your code editor, create a new file at the root level of the repository and name it <code>.env</code>.</p> </li> <li> <p>Add the following environment variables to the <code>.env</code> file. Replace the placeholder values with your actual Google Service Account credentials:</p> </li> </ol> <pre><code>GOOGLE_SERVICE_ACCOUNT_EMAIL=\"your-google-service-account@company.iam.gserviceaccount.com\"\nGOOGLE_SERVICE_ACCOUNT_PRIVATE_KEY=\"-----BEGIN PRIVATE KEY-----&lt;your-private-key&gt;-----END PRIVATE KEY-----\"\n</code></pre>"},{"location":"external_models/surplus_after_refactoring.html#4-start-chap-core","title":"4. Start CHAP Core","text":"<p>At the root level of the repository (the same level you placed the .env-file), run:</p> <pre><code>docker-compose up\n</code></pre> <p>This will build three images and start the following containers:</p> <ul> <li>A REST-API (FastAPI)</li> <li>A Redis server</li> <li>A worker service</li> </ul> <p>You can go to http://localhost:8000/docs to verify that the REST-API is working. A Swagger page, as shown below, should display:</p> <p></p>"},{"location":"external_models/surplus_after_refactoring.html#5-stop-chap-core","title":"5. Stop CHAP Core","text":"<pre><code>docker-compose down\n</code></pre>"},{"location":"external_models/surplus_after_refactoring.html#logs","title":"Logs","text":"<p>When running things with docker compose, some logging will be done by each container. These are written to the <code>logs</code>-directory, and can be useful for debugging purposes:</p> <ul> <li><code>logs/rest_api.log</code>: This contains logs part of the chap-core rest api</li> <li><code>logs/worker.log</code>: This contains logs from the worker running the models. This should be checked if a model for some reason fails</li> <li><code>logs/tas_{task_id}.log</code>: One log file is generated for each task (typically model run). The task_id is the internal task id for the Celery task.</li> </ul>"},{"location":"external_models/surplus_after_refactoring.html#data-requirements-in-chap","title":"Data requirements in CHAP","text":"<p>CHAP expects the data to contain certain features as column names of the supplied csv files. Specifically, time_period, population, disease_cases, location, rainfall and mean_temperature. CHAP gives an error if any of these are missing in the supplied datafile. Additionally, there are conventions for how to represent time in the time_period. For instance, weekly data should be represented as</p> time_period 2014-W52 2015-W01 2015-W02 2015-W03 <p>And for monthly data it should be</p> time_period 2014-12 2015-01 2015-02 2015-03 <p>This requirement is checked for supplied data files for training and predicitng and also for the output data from the model. Additionaly the model should give samples from a distribuition, preferable $1000$ samples for each location and time index with column names <code>sample_0, sample_1</code> and so on.</p> <p>A useful tool for handling data is the adapters that can be included in the MLproject file. These adapters map the internal names in CHAP to whatever you want them to be. For instance disease_cases could be mapped to cases, as in the MLproject file in the repository under the dhis2-chap organization. In practice, the adapters copy the mentioned column and gives it the new column name.</p>"},{"location":"external_models/train_and_predict.html","title":"Making chap-compatible train and predict endpoints","text":"<p>To integrate a component for standardized, interoperable use, it must follow an established standard. CHAP defines one such standard, and by adhering to it, your code gains all the benefits of seamless platform integration. In predictive modeling and machine learning, it is a long-established best practice to provide separate functions for training and prediction.</p> <p>The following figure shows the basic overview of how CHAP expects modelling code to be, i.e. divided into separated train and predict parts</p> <p></p> <p>The figure below shows how the chap platform orchestrates training and prediction of your model using the same endpoints as above:</p> <p></p> <p>The exact way of specifying the train and predict endpoints are described here.</p>"},{"location":"external_models/train_and_predict.html#standardised-data-format","title":"Standardised data format","text":"<p>Part of the standardised interface is to rely on a standardised data format (for the \"historic data\" and \"weather forecast\" data in the figure above). This is a simple csv format. An example is provided in the minimalist_example respository.</p>"},{"location":"external_models/train_and_predict.html#monthly-data-example","title":"Monthly data example","text":"<pre><code>time_period,rainfall,mean_temperature,disease_cases,location\n2023-01,10,30,200,loc1\n2023-02,2,30,100,loc1\n</code></pre>"},{"location":"external_models/train_and_predict.html#weekly-data-example","title":"Weekly data example","text":"<pre><code>time_period,rainfall,mean_temperature,disease_cases,location\n2023-W01,12,28,45,loc1\n2023-W02,8,29,52,loc1\n</code></pre> <p>The <code>time_period</code> column uses: - <code>YYYY-MM</code> format for monthly data (e.g., <code>2023-01</code>) - <code>YYYY-Wnn</code> format for weekly data (e.g., <code>2023-W01</code>)</p>"},{"location":"modeling-app/index.html","title":"Using Chap Core with DHIS2 Modeling App","text":"<p>This section provides documentation for setting up and using Chap Core with the DHIS2 Modeling App.</p> <ul> <li>Installation - How to install and set up the system</li> <li>Server Deployment - Running Chap on a server</li> <li>Using the App - Guide to using the Modeling App</li> </ul>"},{"location":"modeling-app/installation.html","title":"Installing Chap Core for use with the Modeling app","text":"<p>This guide covers installing and running Chap Core using Docker Compose. This is necessary if you want to run the modeling app. Note: For only using chap-core command line interface to run models locally, simply installing chap with <code>pip install chap</code> is sufficient.</p>"},{"location":"modeling-app/installation.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed on your system</li> <li>Git for cloning the repository</li> </ul>"},{"location":"modeling-app/installation.html#fresh-installation-new-users","title":"Fresh Installation (New Users)","text":"<p>Follow these steps if you're installing Chap Core for the first time.</p>"},{"location":"modeling-app/installation.html#1-clone-the-chap-core-repository","title":"1. Clone the Chap Core Repository","text":"<pre><code>git clone https://github.com/dhis2-chap/chap-core.git\ncd chap-core\n</code></pre>"},{"location":"modeling-app/installation.html#2-checkout-the-desired-version","title":"2. Checkout the Desired Version","text":"<p>Fetch the available versions and checkout the version you want to install. Check the releases on GitHub to see what the latest release is.</p> <pre><code># Fetch all tags\ngit fetch --tags\n\n# List available versions\ngit tag -l\n\n# Checkout a specific version (e.g., v1.0.17)\ngit checkout v1.0.17\n</code></pre> <p>To use the latest development version instead, stay on the master branch (skip the checkout step). Note that this branch is unstable and gets frequent changes.</p>"},{"location":"modeling-app/installation.html#3-start-chap-core","title":"3. Start Chap Core","text":"<pre><code>docker compose up\n</code></pre> <p>This single command will: - Pull all required Docker images - Start the PostgreSQL database - Start the Redis cache - Start the Chap Core API server - Start the Celery worker for background jobs - Automatically create and initialize your database</p> <p>The Chap Core REST API will be available at <code>http://localhost:8000</code> once all services are running.</p>"},{"location":"modeling-app/installation.html#4-verify-the-installation","title":"4. Verify the Installation","text":"<p>You can verify that Chap Core is running correctly by:</p> <ol> <li> <p>Check the API documentation: Visit <code>http://localhost:8000/docs</code> in your browser to see the interactive API documentation</p> </li> <li> <p>Check the health endpoint:    <pre><code>curl http://localhost:8000/health\n</code></pre></p> </li> <li> <p>View service logs:    <pre><code>docker compose logs -f\n</code></pre></p> </li> </ol>"},{"location":"modeling-app/installation.html#upgrading-existing-installation","title":"Upgrading Existing Installation","text":"<p>Follow these steps if you already have Chap Core installed and want to upgrade to a newer version.</p>"},{"location":"modeling-app/installation.html#1-backup-your-database-recommended","title":"1. Backup Your Database (Recommended)","text":"<p>Important: Before upgrading, create a backup of your database to prevent data loss in case of issues.</p> <pre><code># Create a backup of the PostgreSQL database\ndocker compose exec -T postgres pg_dump -U root chap_core &gt; backup_$(date +%Y%m%d_%H%M%S).sql\n</code></pre>"},{"location":"modeling-app/installation.html#2-update-the-repository","title":"2. Update the Repository","text":"<pre><code># Navigate to your chap-core directory\ncd chap-core\n\n# Fetch the latest tags and updates\ngit fetch --tags\n\n# List available versions\ngit tag -l\n\n# Checkout the new version you want to upgrade to\ngit checkout v1.0.18  # Replace with your desired version\n</code></pre>"},{"location":"modeling-app/installation.html#3-upgrade-chap-core","title":"3. Upgrade Chap Core","text":"<pre><code># Stop all containers first\ndocker compose down \n\n# Spin the containers up with --build to get new changes\ndocker compose up --build -d\n</code></pre> <p>NOTE: There might be issues with cached images. If you encounter problems, try forcing a fresh pull of all images:</p> <pre><code>docker compose build --no-cache\ndocker compose up -d\n</code></pre> <p>Docke compose up will: - Pull any updated Docker images - Automatically migrate your database to the new schema - Start all services with the new version</p> <p>The database migration happens automatically - you do not need to run any manual migration commands. In the compose.yml file, we pin postgres to a major version (17). Note that between upgrades, there might be minor incompatibilities, such as collation issues. Feel free to handle these by pinning the postgres version further, or handle the database separately.</p>"},{"location":"modeling-app/installation.html#4-verify-the-upgrade","title":"4. Verify the Upgrade","text":"<p>Check that the upgrade was successful, by checking the health endopint of chap locally:</p> <pre><code>curl http://localhost:8000/health\n</code></pre>"},{"location":"modeling-app/installation.html#5-restore-from-backup-if-needed","title":"5. Restore from Backup (If Needed)","text":"<p>If you encounter issues and need to restore from your backup:</p> <pre><code># Stop the services\ndocker compose down\n\n# Remove the database volume to start fresh\ndocker compose down --volumes\n\n# Start only the database\ndocker compose up -d postgres\n\n# Wait for postgres to initialize, then restore the backup\n\ncat backup_20241023_120000.sql | docker compose exec -T postgres psql -U root chap_core\n\n# Start all services\ndocker compose up --build\n</code></pre>"},{"location":"modeling-app/installation.html#common-operations","title":"Common Operations","text":""},{"location":"modeling-app/installation.html#stopping-chap-core","title":"Stopping Chap Core","text":"<p>To stop all services:</p> <pre><code>docker compose down\n</code></pre> <p>This preserves your database data. To start again, simply run <code>docker compose up</code>.</p>"},{"location":"modeling-app/installation.html#viewing-logs","title":"Viewing Logs","text":"<p>To view logs from all services:</p> <pre><code>docker compose logs -f\n</code></pre> <p>To view logs from a specific service:</p> <pre><code>docker compose logs -f chap\ndocker compose logs -f worker\ndocker compose logs -f postgres\n</code></pre>"},{"location":"modeling-app/installation.html#next-steps","title":"Next Steps","text":"<ul> <li>Configure Chap Core to connect with DHIS2 (see Configure DHIS2 Modeling App)</li> <li>Check the Chap Core wiki for more information</li> </ul>"},{"location":"modeling-app/modeling-app.html","title":"Using the Modeling App","text":"<p>This documentation is being updated. For now, please refer to:</p> <ul> <li>Installation - How to install Chap Core for use with the Modeling App</li> <li>Server Deployment - Configure DHIS2 Modeling App and Chap Core to work together</li> </ul> <p>For Modeling App usage instructions, see the app's built-in help or the DHIS2 App Hub.</p>"},{"location":"modeling-app/running-chap-on-server.html","title":"Configure DHIS2 Modeling App and Chap Core to work together","text":"<p>Requirements:</p> <ul> <li>Minimum DHIS2 version: 2.40.7</li> <li>Access to a server running DHIS2</li> </ul> <p>The Modeling App is dependent on connecting to a Chap server, as running the models is a process that needs to be handled by Chap Core. In this tutorial, we will deploy Chap Core to the same server that DHIS2 is running on, and making the Chap Core REST API available internally to the DHIS2 backend. The Modeling App will then use the DHIS2 Route API to connect to the Chap Core endpoint. We will explain the purpose of the Route API later.</p> <p>IMPORTANT: We do not want to make the Chap Core endpoint publicly available on the internet, as Chap Core does not have any method of authenticating requests.</p> <p>NOTE: Previously, Chap Core required you to configure it with Google Earth Engine Credentials. This is not needed anymore, since the Modeling App is now using climate data imported into DHIS2 by the DHIS2 Climate App. Using the DHIS2 Climate App requires you to set up DHIS2 with Google Earth Engine</p>"},{"location":"modeling-app/running-chap-on-server.html#recommendation-around-containerization","title":"Recommendation around containerization","text":"<p>We strongly recommend using Chap Core with a container framework such as LXC, where Chap Core runs in its own environment. Chap Core consists of several different services. In our Chap Core repo, we provide a docker-compose file that containerizes each of these services and makes them work together. We highly recommend you use this docker-compose file when installing Chap Core, since installing each of these services could be very difficult without Docker.</p>"},{"location":"modeling-app/running-chap-on-server.html#lxc-container-setup","title":"LXC container setup","text":"<p>To run Chap Core on a server that is using LXC, you need to create a new LXC container dedicated to Chap Core. Within this LXC container, you then need to install Docker. Ubuntu has documentation on how to install Docker within an LXC container located at https://ubuntu.com/tutorials/how-to-run-docker-inside-lxd-containers#1-overview</p> <p>The Chap Core team has an example of a Chap Core LXC setup, which can be found at https://github.com/dhis2-chap/infrastructure. Handle this code with care. The code is used to deploy Chap Core on a server for conducting integration testing. We recommend everyone only use this as an example and create their own bash script for deploying Chap Core within an LXC container.</p>"},{"location":"modeling-app/running-chap-on-server.html#clone-the-chap-core-repo-into-your-lxc-container","title":"Clone the Chap Core repo into your LXC container","text":"<p>Within your LXC container, you need to clone the Chap Core repo. Information about how to get and start Chap Core, is located at https://github.com/dhis2-chap/chap-core/releases/</p> <p>After you have started Chap Core, the Chap Core REST API will be available at port 8000.</p>"},{"location":"modeling-app/running-chap-on-server.html#identify-the-chap-core-server-private-ip-address-and-verify-chap-core-is-running-properly","title":"Identify the Chap Core server private IP address and verify Chap Core is running properly","text":"<p>We now have Chap Core running in Docker within an LXC container dedicated to Chap Core. Next, you need to identify the private IP address (for instance 192.168.0.174) of the LXC container running Chap Core. This IP address is needed to get the DHIS2 Route API to work. Again, the IP address of Chap Core should not be exposed publicly, only internally on the server. If you are running with LXC, you could, for instance, use lxc list to locate this IP address. You should then use the \"curl\" command to verify that you can connect to the LXC container. In the same terminal as you listed your containers, try to run <code>curl http://[YOUR_IP_ADDRESS]:8000/docs</code> (for instance <code>curl http://192.168.0.174:8000/docs</code>). If Chap Core is running correctly, you should, in response, get some HTML swagger content.</p> <p>Next, verify if you can connect to Chap Core from the container you are running DHIS2 by executing this container and using the curl command (for instance <code>curl http://192.168.0.174:8000/docs</code>).</p>"},{"location":"modeling-app/running-chap-on-server.html#install-the-dhis2-modeling-app","title":"Install the DHIS2 Modeling App","text":"<p>Go into App Management in your DHIS2 instance, and install the Modeling App</p>"},{"location":"modeling-app/running-chap-on-server.html#route-api","title":"Route API","text":"<p>The newer version of DHIS2 supports a built-in reverse proxy named Routes API. This means you could use the DHIS2 backend to forward a request (typically sent from a frontend application) to another IP address (in our case the Chap Core). To use the Route API (the reverse proxy), we need to configure a \"route\" in the DHIS2 backend that forwards requests sent from the Modeling App to the Chap Core. First, we create a Route in DHIS2, which means a resource in DHIS2 that holds information about which IP our \"route\" in DHIS2 should forward requests to.</p> <p>The Modeling App supports a user interface for creating the needed route, where you only need to specify which IP the request should be forwarded to by the route. To create a new route, it requires you to have the DHIS2 System Administrator Role. In the form where you create the route, you need to speficy the IP adress (for instance <code>http://192.168.0.174/**</code>) you located (and verified that was accessible from the DHIS2 container) in the step above.</p> <p>IMPORTANT: You need to configure the route as a \"wildcard route\", by ending the IP with <code>/**</code> More information about Wildcard routes could be found here.</p> <p>IMPORTANT: If you are on version v42 and higher by default the routes will only work for https hosts. If you are using chap in a internal network with SSL, you need to allow http routes to created. More information can be found here. For testing purposes using <code>route.remote_servers_allowed=http://*</code> in your <code>dhis.conf</code> should give you the old behavior.</p>"},{"location":"modeling-app/running-chap-on-server.html#verifying-the-route-is-working-in-the-modeling-app","title":"Verifying the route is working in the Modeling App","text":"<p>You could now go to the Settings page in the Modeling App and verify if the Modeling App can connect to Chap Core. If not, check if the IP address configured in the route is correct and corresponds with the IP you used when you tried to connect to Chap Core from the container you are running DHIS2 in.</p>"},{"location":"webapi/index.html","title":"Chap REST API","text":"<p>This section provides documentation for the Chap REST API.</p> <ul> <li>Developer Guide - Guide for developers working with the API</li> <li>Docker Compose - Setting up the API with Docker Compose</li> </ul>"},{"location":"webapi/developer.html","title":"Using Chap REST API","text":"<p>NOTE: The documentation about how the REST-API is working is limited.</p> <p>There are two main ways to setup the CHAP REST-API:</p> <ul> <li>For local testing purposes, the easiest way is to setup the CHAP REST-API locally with Docker</li> <li>For using Chap in a production environment, it's recommended to instead follow the instructions for setting up the CHAP REST-API on a server</li> </ul>"},{"location":"webapi/docker-compose-doc.html","title":"Setting up Chap REST-API locally","text":"<p>This is a short example for how to setup Chap-core locally as a service using docker-compose.</p> <p>Requirements:</p> <ul> <li>Docker is installed and running on your computer (Installation instructions can be found at https://docs.docker.com/get-started/get-docker/).</li> </ul>"},{"location":"webapi/docker-compose-doc.html#step-by-step-instructions","title":"Step-by-Step Instructions:","text":"<ol> <li> <p>Clone the Chap core repo by running <code>git clone https://github.com/dhis2-chap/chap-core.git</code></p> </li> <li> <p>Run the docker compose file with <code>docker compose -f compose.yml up</code>. The first time you do this, it can take a few minutes to finish. Once it's completed, it should have created the following docker services:</p> </li> <li> <p><code>redis</code> for receiving and queueing job requests</p> </li> <li><code>worker</code> for executing the incoming work requests from queue</li> <li><code>chap</code> containing the main functionality and the rest-api</li> <li> <p><code>postgres</code> for storing chap-related data</p> </li> <li> <p>Check that the chap rest api works by going to http://localhost:8000/docs</p> </li> </ol>"}]}